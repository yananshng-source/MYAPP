# app.py
import streamlit as st
import os
import pandas as pd
from io import BytesIO
from PIL import Image, ImageOps
import requests
from urllib.parse import urljoin
from bs4 import BeautifulSoup
import re
from datetime import datetime
import logging
import traceback
from typing import Iterable, Any
import numpy as np
import subprocess
import sys
import tempfile
import zipfile
import pytesseract
from PIL import ImageEnhance
import pytesseract
import os
from PIL import Image, ImageOps, ImageEnhance
import re
pytesseract.pytesseract.tesseract_cmd = r'E:\tesseract-ocr\tesseract.exe'

# ------------------------ Config ------------------------
st.set_page_config(page_title="ç»¼åˆå¤„ç†å·¥å…·ç®±", layout="wide")
DEFAULT_TIMEOUT = 15
REQUEST_HEADERS = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
VERIFY_SSL = False  # cloud ä¸Šæœ‰äº›ç«™ç‚¹ä¼šè¯ä¹¦é—®é¢˜ï¼Œä¿å®ˆè®¾ä¸º False
MAX_LOG_LINES = 200

# ------------------------ Logging ------------------------
logger = logging.getLogger("ç»¼åˆå¤„ç†å·¥å…·ç®±")
logger.setLevel(logging.INFO)
if not logger.handlers:
    ch = logging.StreamHandler()
    ch.setFormatter(logging.Formatter("%(asctime)s - %(levelname)s - %(message)s"))
    logger.addHandler(ch)

# store recent logs in session state to show in UI
if "recent_logs" not in st.session_state:
    st.session_state.recent_logs = []


def log(msg, level="info"):
    entry = f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - {level.upper()} - {msg}"
    st.session_state.recent_logs.append(entry)
    # cap length
    if len(st.session_state.recent_logs) > MAX_LOG_LINES:
        st.session_state.recent_logs = st.session_state.recent_logs[-MAX_LOG_LINES:]
    if level == "info":
        logger.info(msg)
    elif level == "warning":
        logger.warning(msg)
    elif level == "error":
        logger.error(msg)
    else:
        logger.debug(msg)


# ------------------------ Helpers ------------------------
def progress_iter(it: Iterable[Any], text="å¤„ç†ä¸­...", progress_key=None):
    """
    Generic iterator wrapper that updates a single st.progress bar (main bar).
    It expects an iterable with a determinable length (like list, tuple, DataFrame rows via list()).
    Yields the original items.
    """
    # normalize to list to calculate total reliably (this will hold items in memory)
    # For very large iterables you may replace with a custom strategy.
    items = list(it)
    total = len(items)
    if progress_key is None:
        progress_key = "main_progress"
    progress_bar = st.session_state.get(progress_key)
    if progress_bar is None:
        progress_bar = st.progress(0, text=text)
        st.session_state[progress_key] = progress_bar
    try:
        for idx, item in enumerate(items):
            yield item
            percent = int((idx + 1) / total * 100) if total > 0 else 100
            try:
                progress_bar.progress(percent, text=text)
            except Exception:
                # fallback: ignore progress update error
                pass
        try:
            progress_bar.progress(100, text=text + " âœ… å®Œæˆ")
        except Exception:
            pass
    finally:
        # clear stored progress bar so future calls get a fresh widget
        if progress_key in st.session_state:
            del st.session_state[progress_key]


def safe_requests_get(session: requests.Session, url: str, **kwargs):
    """
    Wrapper around session.get with global headers, timeout, verify options and robust exception handling.
    Returns response or raises.
    """
    try:
        resp = session.get(url, timeout=kwargs.get("timeout", DEFAULT_TIMEOUT),
                           headers=REQUEST_HEADERS, verify=VERIFY_SSL)
        resp.raise_for_status()
        return resp
    except Exception as e:
        raise


def ensure_dir(path):
    os.makedirs(path, exist_ok=True)
    return path

def create_zip_download(files, zip_name="downloaded_images.zip"):
    """åˆ›å»ºZIPæ–‡ä»¶ä¾›ä¸‹è½½"""
    zip_buffer = BytesIO()
    with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
        for file_path in files:
            if os.path.exists(file_path):
                zip_file.write(file_path, os.path.basename(file_path))
    zip_buffer.seek(0)
    return zip_buffer




# ------------------------ Core functions ------------------------
def fix_mojibake(text):
    """ä¿®å¤å¸¸è§çš„ä¹±ç é—®é¢˜"""
    if not isinstance(text, str):
        return text

    # UTF-8å­—èŠ‚è¢«é”™è¯¯è§£ç ä¸ºLatin-1çš„å¸¸è§æƒ…å†µ
    fixes = {
        'ÃƒÆ’Ã‚Â©': 'Ã©', 'ÃƒÆ’Ã‚Â¨': 'Ã¨', 'ÃƒÆ’Ã‚Âª': 'Ãª', 'ÃƒÆ’Ã‚Â§': 'Ã§',
        'ÃƒÆ’Ã‚Â¹': 'Ã¹', 'ÃƒÆ’Ã‚Â»': 'Ã»', 'ÃƒÆ’Ã‚Â®': 'Ã®', 'ÃƒÆ’Ã‚Â¯': 'Ã¯',
        'ÃƒÆ’Ã‚Â´': 'Ã´', 'ÃƒÆ’Ã‚Â¶': 'Ã¶', 'ÃƒÆ’Ã‚Â¼': 'Ã¼', 'ÃƒÆ’Ã‚Â¤': 'Ã¤',
        'ÃƒÆ’Ã‚Â¥': 'Ã¥', 'ÃƒÆ’Ã‚Â¦': 'Ã¦', 'ÃƒÆ’Ã‚Â¸': 'Ã¸', 'ÃƒÆ’Ã‚Â¿': 'Ã¿',
        'ÃƒÂ©': 'Ã©', 'ÃƒÂ¨': 'Ã¨', 'ÃƒÂª': 'Ãª', 'ÃƒÂ§': 'Ã§',
        'ÃƒÂ¹': 'Ã¹', 'ÃƒÂ»': 'Ã»', 'ÃƒÂ®': 'Ã®', 'ÃƒÂ¯': 'Ã¯',
        'ÃƒÂ´': 'Ã´', 'ÃƒÂ¶': 'Ã¶', 'ÃƒÂ¼': 'Ã¼', 'ÃƒÂ¤': 'Ã¤',
        'ÃƒÂ¥': 'Ã¥', 'ÃƒÂ¦': 'Ã¦', 'ÃƒÂ¸': 'Ã¸', 'ÃƒÂ¿': 'Ã¿',
        'Ã¢â‚¬Â¢': 'Â·', 'Ã¢â‚¬"': 'â€”', 'Ã¢â‚¬Â¦': 'â€¦', 'Ã¢â‚¬Ëœ': "'",
        'Ã¢â‚¬â„¢': "'", 'Ã¢â‚¬Å“': '"', 'Ã¢â‚¬': '"', 'Ã¢â‚¬â€': 'â€”',
        'Ã¢â‚¬"': 'â€”', 'Ã¢â‚¬"': 'â€”', 'Ã¢â‚¬"': 'â€”',
        'Ã‚': '', 'Ã‚ ': ' ', 'Ã‚Â ': ' ',  # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        'Ã¥': 'â€¢', 'Ã¦': 'â€¢', 'Ã¨': 'Â·', 'Ã©': 'Â·',
        'Â¡Â¯': "'", 'Â¡Â±': '"', 'Â¡Â°': '"',
        'Ã¯Â¼Ë†': 'ï¼ˆ', 'Ã¯Â¼â€°': 'ï¼‰', 'Ã¯Â¼Å¡': 'ï¼š',
        'Ã¯Â¼Å’': 'ï¼Œ', 'Ã¯Â¼Â': 'ï¼', 'Ã¯Â¼Å¸': 'ï¼Ÿ',
        'Ã¯Â¼â€º': 'ï¼›', 'Ã¯Â¼â‚¬': 'ï¿¥'
    }

    for wrong, right in fixes.items():
        text = text.replace(wrong, right)

    return text


def clean_dataframe_encoding(df):
    """æ¸…ç†DataFrameä¸­çš„ç¼–ç é—®é¢˜"""
    df_clean = df.copy()

    for col in df_clean.columns:
        if df_clean[col].dtype == 'object':
            # å°è¯•æ¸…ç†å­—ç¬¦ä¸²
            df_clean[col] = df_clean[col].apply(
                lambda x: fix_mojibake(x) if isinstance(x, str) else x
            )

    return df_clean


def scrape_table(url_list, group_cols):
    """
    ä¿®å¤ç¼–ç é—®é¢˜çš„ç½‘é¡µè¡¨æ ¼æŠ“å–
    """
    session = requests.Session()
    sheet_data = {}
    all_data = []
    errors = []

    enumerated = list(enumerate(url_list, start=1))
    for idx, url in progress_iter(enumerated, text="æŠ“å–ç½‘é¡µè¡¨æ ¼ä¸­"):
        try:
            _, page_url = (idx, url)
            log(f"æ­£åœ¨æŠ“å–: {page_url}")
            resp = safe_requests_get(session, page_url)

            # ä¿å­˜åŸå§‹å†…å®¹ç”¨äºç¼–ç æ£€æµ‹
            original_content = resp.content

            # è‡ªåŠ¨æ£€æµ‹ç¼–ç 
            if resp.encoding is None or resp.encoding.lower() == 'iso-8859-1':
                resp.encoding = resp.apparent_encoding

            text = resp.text
            log(f"åˆå§‹ç¼–ç : {resp.encoding}, å†…å®¹é•¿åº¦: {len(text)}")

            # æ£€æµ‹ä¹±ç ç‰¹å¾
            mojibake_patterns = ['Ãƒ', 'Ã¢â‚¬', 'Ã¥', 'Ã¦', 'Ã¨', 'Ã©', 'Ã¯Â¼']
            has_mojibake = any(pattern in text for pattern in mojibake_patterns)

            if has_mojibake:
                log(f"æ£€æµ‹åˆ°ä¹±ç ï¼Œå°è¯•ä¿®å¤...")
                # å°è¯•å¸¸è§ä¸­æ–‡ç¼–ç 
                encodings_to_try = ['gbk', 'gb2312', 'gb18030', 'big5', 'utf-8']

                for encoding in encodings_to_try:
                    try:
                        # ä½¿ç”¨æ–°ç¼–ç é‡æ–°è§£ç 
                        decoded_text = original_content.decode(encoding, errors='ignore')
                        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ä¹±ç 
                        if not any(pattern in decoded_text for pattern in mojibake_patterns):
                            text = decoded_text
                            log(f"âœ… ä½¿ç”¨ {encoding} ç¼–ç æˆåŠŸè§£å†³ä¹±ç ")
                            break
                        else:
                            log(f"âŒ {encoding} ç¼–ç ä»æœ‰ä¹±ç ")
                    except Exception as e:
                        log(f"å°è¯•ç¼–ç  {encoding} å¤±è´¥: {e}", level="debug")
                        continue

            try:
                dfs = pd.read_html(text)
                log(f"æˆåŠŸè¯»å– {len(dfs)} ä¸ªè¡¨æ ¼")
            except Exception as e:
                msg = f"read_html å¤±è´¥: {page_url} -> {e}"
                log(msg, level="warning")
                errors.append(msg)
                # å°è¯•ä½¿ç”¨å­—èŠ‚å†…å®¹è¯»å–
                try:
                    log("å°è¯•ä½¿ç”¨å­—èŠ‚å†…å®¹è¯»å–è¡¨æ ¼...")
                    dfs = pd.read_html(original_content)
                    log(f"å­—èŠ‚å†…å®¹è¯»å–æˆåŠŸ: {len(dfs)} ä¸ªè¡¨æ ¼")
                except Exception as e2:
                    log(f"å­—èŠ‚å†…å®¹è¯»å–ä¹Ÿå¤±è´¥: {e2}", level="warning")
                    continue

            for i, df in enumerate(dfs):
                # æ¸…ç†DataFrameä¸­çš„ä¹±ç 
                df_clean = clean_dataframe_encoding(df)
                name = f"ç½‘é¡µ{idx}_è¡¨{i + 1}"
                sheet_data[name] = df_clean
                all_data.append(df_clean)
                log(f"âœ… æŠ“å–åˆ°è¡¨æ ¼: {name} ({len(df_clean)} è¡Œ)")

                # æ˜¾ç¤ºè¡¨æ ¼é¢„è§ˆä¿¡æ¯
                if len(df_clean) > 0:
                    log(f"ğŸ“Š è¡¨æ ¼é¢„è§ˆ - åˆ—: {list(df_clean.columns)}")
                    if len(df_clean) >= 1:
                        sample_data = df_clean.iloc[0].to_dict()
                        log(f"ğŸ“ é¦–è¡Œæ ·ä¾‹: {str(sample_data)[:100]}...")

        except Exception as e:
            error_msg = f"âŒ æŠ“å– URL å¤±è´¥: {url} -> {repr(e)}"
            log(error_msg, level="warning")
            errors.append(error_msg)
            continue

    if sheet_data:
        output = BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            for name, df in sheet_data.items():
                safe_name = name[:31]
                df.to_excel(writer, sheet_name=safe_name, index=False)
                log(f"ğŸ’¾ å†™å…¥å·¥ä½œè¡¨: {safe_name}")

            if all_data:
                try:
                    combined_df = pd.concat(all_data, ignore_index=True)
                    combined_df.to_excel(writer, sheet_name="æ±‡æ€»", index=False)
                    log(f"ğŸ“‹ åˆ›å»ºæ±‡æ€»è¡¨: {len(combined_df)} è¡Œ")
                except Exception as e:
                    log(f"åˆå¹¶æ±‡æ€»è¡¨å¤±è´¥: {e}", level="warning")

        output.seek(0)

        # è®°å½•æœ€ç»ˆç»“æœ
        total_tables = len(sheet_data)
        total_rows = sum(len(df) for df in sheet_data.values())
        log(f"ğŸ‰ æŠ“å–å®Œæˆ: {total_tables} ä¸ªè¡¨æ ¼, {total_rows} è¡Œæ•°æ®")

        return output
    else:
        log("âŒ æœªæŠ“å–åˆ°ä»»ä½•è¡¨æ ¼ã€‚", level="warning")
        return None


def download_images_from_urls(url_list, output_dir=None):
    """
    ä»æ¯ä¸ªé¡µé¢æŠ“å– <img> å¹¶ä¸‹è½½ã€‚
    è¿”å› (output_dir, downloaded_file_paths, errors)
    """
    # åœ¨äº‘ç¯å¢ƒä¸­ä½¿ç”¨ä¸´æ—¶ç›®å½•
    if output_dir is None:
        # å°è¯•åˆ›å»ºæ¡Œé¢ç›®å½•ï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨ä¸´æ—¶ç›®å½•
        try:
            desktop_path = os.path.join(os.path.expanduser("~"), "Desktop", "downloaded_images")
            ensure_dir(desktop_path)
            # æµ‹è¯•å†™å…¥æƒé™
            test_file = os.path.join(desktop_path, "test_write.txt")
            with open(test_file, 'w') as f:
                f.write("test")
            os.remove(test_file)
            output_dir = desktop_path
        except (PermissionError, OSError):
            # å¦‚æœæ²¡æœ‰æ¡Œé¢å†™å…¥æƒé™ï¼Œä½¿ç”¨ä¸´æ—¶ç›®å½•
            output_dir = os.path.join(tempfile.gettempdir(), "downloaded_images")
            ensure_dir(output_dir)

    log(f"ğŸ“ å›¾ç‰‡ä¸‹è½½ç›®å½•: {output_dir}")

    session = requests.Session()
    session.headers.update(REQUEST_HEADERS)
    downloaded_files = []
    errors = []

    enumerated = list(enumerate(url_list, start=1))
    for idx, url in progress_iter(enumerated, text="ä¸‹è½½ç½‘é¡µå›¾ç‰‡ä¸­"):
        try:
            _, page_url = (idx, url)
            log(f"æ­£åœ¨è®¿é—®: {page_url}")
            resp = safe_requests_get(session, page_url)
            soup = BeautifulSoup(resp.content, "html.parser")
            title_tag = soup.find("title")
            title = title_tag.string.strip() if title_tag and title_tag.string else f"ç½‘é¡µ{idx}"
            safe_title = "".join([c if c not in r'\/:*?"<>|' else "_" for c in title])

            imgs = soup.find_all("img")
            log(f"ğŸ“„ {page_url} - æ‰¾åˆ° {len(imgs)} å¼ å›¾ç‰‡")

            if not imgs:
                log(f"{page_url} - æœªæ‰¾åˆ° img æ ‡ç­¾", level="info")
                continue

            for i, img_tag in enumerate(imgs, start=1):
                src = img_tag.get("src") or img_tag.get("data-src") or img_tag.get("data-original")
                if not src:
                    continue

                full_url = urljoin(page_url, src.strip())
                log(f"æ­£åœ¨ä¸‹è½½å›¾ç‰‡: {full_url}")

                try:
                    resp_img = safe_requests_get(session, full_url)

                    # æ–‡ä»¶æ‰©å±•åå¤„ç†
                    ext = os.path.splitext(full_url.split('?')[0])[1]
                    if not ext or len(ext) > 6:
                        content_type = resp_img.headers.get('content-type', '')
                        if 'jpeg' in content_type or 'jpg' in content_type:
                            ext = ".jpg"
                        elif 'png' in content_type:
                            ext = ".png"
                        elif 'gif' in content_type:
                            ext = ".gif"
                        else:
                            ext = ".jpg"

                    fname = f"{safe_title}_{i:02d}{ext}"
                    fpath = os.path.join(output_dir, fname)

                    # é¿å…æ–‡ä»¶åé‡å¤
                    counter = 1
                    original_fpath = fpath
                    while os.path.exists(fpath):
                        name_only = os.path.splitext(original_fpath)[0]
                        fpath = f"{name_only}_{counter}{ext}"
                        counter += 1

                    with open(fpath, "wb") as f:
                        f.write(resp_img.content)

                    downloaded_files.append(fpath)
                    log(f"âœ… ä¸‹è½½æˆåŠŸ: {os.path.basename(fpath)} - å¤§å°: {len(resp_img.content)} bytes")

                except Exception as e:
                    error_msg = f"å›¾ç‰‡ä¸‹è½½å¤±è´¥: {full_url} -> {repr(e)}"
                    errors.append(error_msg)
                    log(error_msg, level="warning")
                    continue

        except Exception as e:
            error_msg = f"é¡µé¢è¯·æ±‚å¤±è´¥: {url} -> {repr(e)}"
            log(error_msg, level="warning")
            errors.append(error_msg)
            continue

    log(f"ğŸ‰ ä¸‹è½½å®Œæˆ! æ€»å…±ä¸‹è½½ {len(downloaded_files)} å¼ å›¾ç‰‡åˆ° {output_dir}")
    return output_dir, downloaded_files, errors


def crop_images_only(folder_path, x_center, y_center, crop_width, crop_height):
    output_folder = os.path.join(os.path.expanduser("~"), "Desktop", "crop_results")
    ensure_dir(output_folder)
    img_exts = (".png", ".jpg", ".jpeg", ".bmp", ".tif", ".tiff")
    filenames = [f for f in os.listdir(folder_path) if f.lower().endswith(img_exts)]
    for filename in progress_iter(filenames, text="è£å‰ªå›¾ç‰‡ä¸­"):
        try:
            image_path = os.path.join(folder_path, filename)
            img = Image.open(image_path).convert("RGB")
            width, height = img.size
            left = max(0, int(x_center - crop_width // 2))
            right = min(width, int(x_center + crop_width // 2))
            top = max(0, int(y_center - crop_height // 2))
            bottom = min(height, int(y_center + crop_height // 2))
            crop_img = img.crop((left, top, right, bottom))
            # æ”¾å¤§äºŒå€ç”¨äºåç»­è¯†åˆ«/æŸ¥çœ‹
            crop_img = crop_img.resize((crop_img.width * 2, crop_img.height * 2), Image.LANCZOS)
            bw = ImageOps.grayscale(crop_img)
            save_path = os.path.join(output_folder, f"crop_{filename}")
            bw.save(save_path)
            log(f"è£å‰ªå¹¶ä¿å­˜: {save_path}")
        except Exception as e:
            log(f"è£å‰ªå¤±è´¥: {filename} -> {e}", level="warning")
            continue
    return output_folder


# ------------------------ é€‰ç§‘è½¬æ¢ä¸æ—¥æœŸå¤„ç† helpers ------------------------
def convert_selection_requirements(df):
    subject_mapping = {'ç‰©ç†': 'ç‰©', 'åŒ–å­¦': 'åŒ–', 'ç”Ÿç‰©': 'ç”Ÿ', 'å†å²': 'å†', 'åœ°ç†': 'åœ°', 'æ”¿æ²»': 'æ”¿',
                       'æ€æƒ³æ”¿æ²»': 'æ”¿'}
    df_new = df.copy()
    df_new['é¦–é€‰ç§‘ç›®'] = ''
    df_new['é€‰ç§‘è¦æ±‚ç±»å‹'] = ''
    df_new['æ¬¡é€‰'] = ''

    # iterate rows - we selected "row" granular progress behavior
    total_rows = len(df)
    for idx, row in progress_iter(list(df.iterrows()), text="é€‰ç§‘è½¬æ¢ä¸­"):
        try:
            i, r = row
            text = str(r.get('é€‰ç§‘è¦æ±‚', '')).strip()
            cat = str(r.get('æ‹›ç”Ÿç§‘ç±»', '')).strip()
            subjects = [subject_mapping.get(s, s) for s in
                        re.findall(r'ç‰©ç†|åŒ–å­¦|ç”Ÿç‰©|å†å²|åœ°ç†|æ”¿æ²»|æ€æƒ³æ”¿æ²»', text)]
            first = ''
            for s_full, s_short in subject_mapping.items():
                if f'é¦–é€‰{s_full}' in text:
                    first = s_short
            if not first:
                if 'ç‰©ç†' in cat:
                    first = 'ç‰©'
                elif 'å†å²' in cat:
                    first = 'å†'
            remaining = [s for s in subjects if s != first]
            second = ''.join(remaining)
            if 'ä¸é™' in text:
                req_type = 'ä¸é™ç§‘ç›®ä¸“ä¸šç»„'
            elif len(remaining) >= 1:
                req_type = 'å¤šé—¨é€‰è€ƒ'
            else:
                req_type = 'å•ç§‘ã€å¤šç§‘å‡éœ€é€‰è€ƒ'
            df_new.at[i, 'é¦–é€‰ç§‘ç›®'] = first
            df_new.at[i, 'æ¬¡é€‰'] = second
            df_new.at[i, 'é€‰ç§‘è¦æ±‚ç±»å‹'] = req_type
        except Exception as e:
            log(f"é€‰ç§‘è¡Œå¤„ç†å¤±è´¥: idx={i} -> {e}", level="warning")
            continue
    return df_new


def safe_parse_datetime(datetime_str, year):
    if pd.isna(datetime_str):
        return None
    datetime_str = str(datetime_str).strip()
    if not re.search(r'(^|\D)\d{4}(\D|$)', datetime_str):
        datetime_str = f"{year}å¹´{datetime_str}"
    patterns = [(r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥(\d{1,2}):(\d{1,2})', '%Yå¹´%mæœˆ%dæ—¥%H:%M'),
                (r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥', '%Yå¹´%mæœˆ%dæ—¥'),
                (r'(\d{4})-(\d{1,2})-(\d{1,2})', '%Y-%m-%d'),
                (r'(\d{4})/(\d{1,2})/(\d{1,2})', '%Y/%m/%d')]
    for pattern, fmt in patterns:
        try:
            dt = datetime.strptime(datetime_str, fmt)
            return dt
        except Exception:
            continue
    return None


def process_date_range(date_str, year):
    if pd.isna(date_str):
        return date_str, "", ""
    date_str = str(date_str).strip()
    if '-' in date_str:
        start_str, end_str = date_str.split('-', 1)
        start_dt = safe_parse_datetime(start_str, year)
        end_dt = safe_parse_datetime(end_str, year)
        if not start_dt or not end_dt:
            return date_str, "æ ¼å¼é”™è¯¯", "æ ¼å¼é”™è¯¯"
        if ':' not in start_str:
            start_dt = start_dt.replace(hour=0, minute=0, second=0)
        if ':' not in end_str:
            end_dt = end_dt.replace(hour=23, minute=59, second=59)
        if end_dt < start_dt:
            # assume cross-year, å°è¯•å°†ç»“æŸå¹´è®¾åˆ°ä¸‹ä¸€å¹´
            try:
                end_dt = end_dt.replace(year=start_dt.year + 1)
            except Exception:
                pass
        return date_str, start_dt.strftime('%Y-%m-%d %H:%M:%S'), end_dt.strftime('%Y-%m-%d %H:%M:%S')
    else:
        dt = safe_parse_datetime(date_str, year)
        if not dt:
            return date_str, "æ ¼å¼é”™è¯¯", "æ ¼å¼é”™è¯¯"
        start_dt = dt.replace(hour=0, minute=0, second=0) if ':' not in date_str else dt
        end_dt = dt.replace(hour=23, minute=59, second=59) if ':' not in date_str else dt
        return date_str, start_dt.strftime('%Y-%m-%d %H:%M:%S'), end_dt.strftime('%Y-%m-%d %H:%M:%S')


# ------------------------ æ£€æŸ¥Tesseractå®‰è£… ------------------------
def check_tesseract_installation():
    """æ£€æŸ¥Tesseractæ˜¯å¦å®‰è£…"""
    try:
        # å°è¯•è·å–Tesseractç‰ˆæœ¬
        pytesseract.get_tesseract_version()
        return True, "Tesseract OCRå·²å®‰è£…"
    except Exception as e:
        return False, f"Tesseract OCRæœªå®‰è£…æˆ–è·¯å¾„é”™è¯¯: {e}"



# ------------------------ Streamlit UI ------------------------
st.title("ğŸ§° ç»¼åˆå¤„ç†å·¥å…·ç®± - å®Œæ•´ç‰ˆï¼ˆå¸¦è¿›åº¦æ¡ & æ—¥å¿—ï¼‰")
tab1, tab2, tab3, tab4, tab5 = st.tabs([
    "ç½‘é¡µè¡¨æ ¼æŠ“å–",
    "ç½‘é¡µå›¾ç‰‡ä¸‹è½½",
    "Excelæ—¥æœŸå¤„ç†",
    "åˆ†æ•°åŒ¹é…",
    "å­¦ä¸šæ¡¥-é«˜è€ƒä¸“ä¸šåˆ†æ•°æ®è½¬æ¢"
])

# side: logs
with st.sidebar.expander("è¿è¡Œæ—¥å¿—ï¼ˆæœ€æ–°ï¼‰", expanded=True):
    for line in st.session_state.recent_logs[-200:]:
        st.text(line)

# ------------------------ Tab 1: ç½‘é¡µè¡¨æ ¼æŠ“å– ------------------------
with tab1:
    st.subheader("ç½‘é¡µè¡¨æ ¼æŠ“å–")
    urls_text = st.text_area("è¾“å…¥ç½‘é¡µURLåˆ—è¡¨ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰", height=160,
                             placeholder="ä¾‹å¦‚:\nhttps://example.com/table1\nhttps://example.com/table2")
    group_cols = st.text_input("åˆ†ç»„åˆ—ï¼ˆé€—å·åˆ†éš”ï¼Œå¯é€‰ï¼‰",
                               placeholder="ä¾‹å¦‚: çœä»½,æ‰¹æ¬¡,ç§‘ç±»")

    # æ·»åŠ è°ƒè¯•é€‰é¡¹
    with st.expander("ğŸ”§ é«˜çº§é€‰é¡¹", expanded=False):
        debug_mode = st.checkbox("å¯ç”¨è°ƒè¯•æ¨¡å¼", value=True,
                                 help="æ˜¾ç¤ºè¯¦ç»†çš„å¤„ç†æ—¥å¿—å’Œç¼–ç ä¿¡æ¯")
        show_preview = st.checkbox("æ˜¾ç¤ºè¡¨æ ¼é¢„è§ˆ", value=True,
                                   help="åœ¨æ—¥å¿—ä¸­æ˜¾ç¤ºè¡¨æ ¼çš„å‰å‡ è¡Œæ•°æ®")

    col1, col2 = st.columns([1, 3])
    with col1:
        if st.button("ğŸš€ å¼€å§‹æŠ“å–è¡¨æ ¼", key="scrape", type="primary"):
            url_list = [u.strip() for u in urls_text.splitlines() if u.strip()]
            if not url_list:
                st.warning("è¯·å…ˆè¾“å…¥æœ‰æ•ˆURLåˆ—è¡¨")
            else:
                try:
                    # æ˜¾ç¤ºå¤„ç†çŠ¶æ€
                    status_placeholder = st.empty()
                    progress_placeholder = st.empty()
                    result_placeholder = st.empty()

                    status_placeholder.info(f"ğŸ”„ å¼€å§‹æŠ“å– {len(url_list)} ä¸ªç½‘é¡µ...")

                    # å¼€å§‹æŠ“å–
                    with progress_placeholder.container():
                        output = scrape_table(url_list, group_cols)

                    if output:
                        status_placeholder.success("âœ… æŠ“å–å®Œæˆï¼")

                        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                        total_size = len(output.getvalue()) / 1024  # KB
                        result_placeholder.success(
                            f"**æŠ“å–ç»“æœ:**\n"
                            f"- ç”ŸæˆExcelæ–‡ä»¶å¤§å°: {total_size:.1f} KB\n"
                            f"- åŒ…å« {len([k for k in st.session_state.recent_logs if 'æŠ“å–åˆ°è¡¨æ ¼' in k])} ä¸ªè¡¨æ ¼\n"
                            f"- æŸ¥çœ‹ä¾§è¾¹æ æ—¥å¿—äº†è§£è¯¦ç»†ä¿¡æ¯"
                        )

                        # æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯
                        if debug_mode:
                            debug_expander = st.expander("ğŸ“‹ è¯¦ç»†å¤„ç†æ—¥å¿—", expanded=False)
                            with debug_expander:
                                # æ˜¾ç¤ºç›¸å…³çš„å¤„ç†æ—¥å¿—
                                relevant_logs = [
                                    log for log in st.session_state.recent_logs
                                    if any(keyword in log for keyword in [
                                        'æ­£åœ¨æŠ“å–', 'åˆå§‹ç¼–ç ', 'æ£€æµ‹åˆ°ä¹±ç ', 'ä½¿ç”¨ç¼–ç ',
                                        'æˆåŠŸè¯»å–', 'æŠ“å–åˆ°è¡¨æ ¼', 'è¡¨æ ¼é¢„è§ˆ'
                                    ])
                                ]
                                for log_entry in relevant_logs[-20:]:  # æ˜¾ç¤ºæœ€è¿‘20æ¡ç›¸å…³æ—¥å¿—
                                    st.text(log_entry)

                        # ä¸‹è½½æŒ‰é’®
                        st.download_button(
                            "ğŸ“¥ ä¸‹è½½æŠ“å–è¡¨æ ¼",
                            data=output.getvalue(),
                            file_name=f"ç½‘é¡µæŠ“å–_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
                            help="åŒ…å«æ‰€æœ‰æŠ“å–åˆ°çš„è¡¨æ ¼å’Œæ±‡æ€»è¡¨",
                            type="primary"
                        )
                    else:
                        status_placeholder.warning("âš ï¸ æœªæŠ“å–åˆ°è¡¨æ ¼æ•°æ®")
                        # æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
                        error_logs = [log for log in st.session_state.recent_logs
                                      if "å¤±è´¥" in log or "é”™è¯¯" in log or "âŒ" in log]
                        if error_logs:
                            st.error("âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°ä»¥ä¸‹é—®é¢˜:")
                            for error in error_logs[-10:]:
                                st.text(error)

                except Exception as e:
                    log(f"âŒ æŠ“å–è¡¨æ ¼æ€»æµç¨‹å¤±è´¥: {e}", level="error")
                    st.error(f"âŒ æŠ“å–è¡¨æ ¼å‡ºé”™: {str(e)}")
                    # æ˜¾ç¤ºè¯¦ç»†é”™è¯¯
                    if debug_mode:
                        with st.expander("ğŸ” é”™è¯¯è¯¦æƒ…", expanded=False):
                            st.code(traceback.format_exc())

# ------------------------ Tab 2: ç½‘é¡µå›¾ç‰‡ä¸‹è½½ ------------------------
with tab2:
    st.subheader("ç½‘é¡µå›¾ç‰‡ä¸‹è½½")
    urls_text2 = st.text_area("è¾“å…¥ç½‘é¡µURLåˆ—è¡¨ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰", height=160, key="img_urls")

    # æ˜¾ç¤ºå½“å‰å·¥ä½œç›®å½•ä¿¡æ¯
    st.info(f"å½“å‰å·¥ä½œç›®å½•: `{os.getcwd()}`")
    st.info(f"ä¸´æ—¶æ–‡ä»¶ç›®å½•: `{tempfile.gettempdir()}`")

    col1, col2 = st.columns([1, 3])
    with col1:
        if st.button("ä¸‹è½½å›¾ç‰‡", key="img_download"):
            url_list = [u.strip() for u in urls_text2.splitlines() if u.strip()]
            if not url_list:
                st.warning("è¯·å…ˆè¾“å…¥æœ‰æ•ˆURLåˆ—è¡¨")
            else:
                try:
                    output_dir, files, errors = download_images_from_urls(url_list)

                    # æ˜¾ç¤ºä¸‹è½½ç»“æœ
                    st.success(f"âœ… å®Œæˆï¼å…±ä¸‹è½½ {len(files)} å¼ å›¾ç‰‡")
                    st.success(f"ğŸ“ ä¿å­˜åˆ°: `{output_dir}`")

                    # æ˜¾ç¤ºæ–‡ä»¶åˆ—è¡¨
                    if files:
                        st.subheader("ğŸ“„ ä¸‹è½½çš„æ–‡ä»¶åˆ—è¡¨:")

                        # åˆ›å»ºZIPä¸‹è½½
                        zip_buffer = create_zip_download(files)
                        st.download_button(
                            label="ğŸ“¦ ä¸‹è½½æ‰€æœ‰å›¾ç‰‡(ZIP)",
                            data=zip_buffer.getvalue(),
                            file_name="downloaded_images.zip",
                            mime="application/zip"
                        )

                        # æ˜¾ç¤ºæ–‡ä»¶è¯¦æƒ…å’Œé¢„è§ˆ
                        for i, file_path in enumerate(files, 1):
                            file_name = os.path.basename(file_path)
                            file_size = os.path.getsize(file_path) if os.path.exists(file_path) else 0

                            col1, col2 = st.columns([3, 1])
                            with col1:
                                st.write(f"{i}. **{file_name}** ({file_size} bytes)")
                            with col2:
                                # å•ä¸ªæ–‡ä»¶ä¸‹è½½
                                with open(file_path, 'rb') as f:
                                    st.download_button(
                                        f"ä¸‹è½½{i}",
                                        f.read(),
                                        file_name=file_name,
                                        key=f"single_{i}"
                                    )

                            # å›¾ç‰‡é¢„è§ˆ
                            try:
                                st.image(file_path, caption=file_name, width=300)
                            except Exception as e:
                                st.write(f"é¢„è§ˆå¤±è´¥: {e}")

                    if errors:
                        st.warning(f"æœ‰ {len(errors)} ä¸ªé”™è¯¯:")
                        for error in errors[-5:]:
                            st.error(error)

                except Exception as e:
                    log(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥: {e}\n{traceback.format_exc()}", level="error")
                    st.error(f"ä¸‹è½½å›¾ç‰‡å‡ºé”™: {e}")



# ------------------------ Tab 3: Excelæ—¥æœŸå¤„ç† ------------------------
with tab3:
    st.subheader("Excelæ—¥æœŸå¤„ç†")
    uploaded_file2 = st.file_uploader("ä¸Šä¼ Excelæ–‡ä»¶", type=["xlsx", "xls"], key="date_excel")
    year = st.number_input("å¹´ä»½ï¼ˆç”¨äºè¡¥å…¨ï¼‰", value=datetime.now().year, key="date_year")
    date_col = st.text_input("æ—¥æœŸåˆ—å", value="æ—¥æœŸ", key="date_col")

    if uploaded_file2:
        try:
            df2 = pd.read_excel(uploaded_file2)
            st.write("åŸå§‹æ•°æ®é¢„è§ˆ", df2.head())
            if st.button("å¤„ç†æ—¥æœŸ", key="date_btn"):
                try:
                    start_times = []
                    end_times = []
                    originals = []
                    # row-by-row processing (you selected 'row' granular mode)
                    for d in progress_iter(list(df2[date_col]), text="æ—¥æœŸå¤„ç†ä¸­"):
                        orig, start, end = process_date_range(d, int(year))
                        originals.append(orig)
                        start_times.append(start)
                        end_times.append(end)
                    df2_result = df2.copy()
                    insert_at = df2_result.columns.get_loc(date_col) + 1
                    df2_result.insert(insert_at, 'å¼€å§‹æ—¶é—´', start_times)
                    df2_result.insert(insert_at + 1, 'ç»“æŸæ—¶é—´', end_times)
                    st.write("å¤„ç†ç»“æœé¢„è§ˆ", df2_result.head())
                    towrite2 = BytesIO()
                    df2_result.to_excel(towrite2, index=False)
                    towrite2.seek(0)
                    st.download_button("ä¸‹è½½æ—¥æœŸå¤„ç†ç»“æœExcel", data=towrite2.getvalue(), file_name="æ—¥æœŸå¤„ç†ç»“æœ.xlsx")
                    st.success("æ—¥æœŸå¤„ç†å®Œæˆ")
                except Exception as e:
                    log(f"æ—¥æœŸå¤„ç†å¤±è´¥: {e}\n{traceback.format_exc()}", level="error")
                    st.error("æ—¥æœŸå¤„ç†å‡ºé”™ï¼Œè¯¦æƒ…è§æ—¥å¿—")
        except Exception as e:
            log(f"è¯»å–ä¸Šä¼ æ–‡ä»¶å¤±è´¥: {e}", level="error")
            st.error("æ— æ³•è¯»å–ä¸Šä¼ çš„ Excel æ–‡ä»¶")

    # =====================================================
    # ======================= TAB 4 =======================
    # =====================================================
with tab4:
    st.header("ğŸ“ æ‹›ç”Ÿè®¡åˆ’ & åˆ†æ•°è¡¨ æ™ºèƒ½åŒ¹é…å·¥å…·")

    # ================= åŒ¹é…é…ç½® =================
    MATCH_KEYS = ["å­¦æ ¡", "çœä»½", "ç§‘ç±»", "å±‚æ¬¡", "æ‰¹æ¬¡", "æ‹›ç”Ÿç±»å‹", "ä¸“ä¸š"]

    # âš  åªç”¨äºã€äººå·¥é‡å¤åŒ¹é…åŒºã€‘å€™é€‰é¡¹å±•ç¤º
    DISPLAY_FIELDS = [
        "å­¦æ ¡",
        "çœä»½",
        "ç§‘ç±»",
        "æ‰¹æ¬¡",
        "ä¸“ä¸š",
        "å¤‡æ³¨",
        "æ‹›ç”Ÿç±»å‹"
    ]

    TEXT_COLUMNS = {"ä¸“ä¸šç»„ä»£ç ", "æ‹›ç”Ÿä»£ç ", "ä¸“ä¸šä»£ç "}


    # ================= å·¥å…·å‡½æ•° =================
    def normalize(df: pd.DataFrame) -> pd.DataFrame:
        df = df.copy()
        for col in MATCH_KEYS:
            if col in df.columns:
                df[col + "_norm"] = (
                    df[col]
                    .fillna("")  # â­ å¡«å……ç©ºå€¼
                    .astype(str)  # â­ å¼ºåˆ¶è½¬ä¸ºå­—ç¬¦ä¸²
                    .str.strip()
                    .str.replace("\u3000", "")
                    .str.lower()
                )

        if "å±‚æ¬¡_norm" in df.columns:
            df["å±‚æ¬¡_norm"] = df["å±‚æ¬¡_norm"].replace({
                "ä¸“ç§‘": "ä¸“ç§‘(é«˜èŒ)"
            })

        return df


    def build_key(df: pd.DataFrame) -> pd.Series:
        norm_cols = [c + "_norm" for c in MATCH_KEYS if c + "_norm" in df.columns]
        if not norm_cols:  # âš  å…œåº•ï¼šå¦‚æœæ²¡æœ‰_normåˆ—
            return pd.Series([""] * len(df), index=df.index)
        return df[norm_cols].fillna("").astype(str).agg("||".join, axis=1)


    def safe_text(v):
        return str(v) if pd.notna(v) and str(v).strip() else ""


    def clean_code_text(v):
        if pd.isna(v):
            return ""
        s = str(v).strip()
        if s.startswith("^"):
            s = s[1:]
        return s


    # ================= é€‰ç§‘è§£æ =================
    def calc_first_subject(kl: str) -> str:
        if not isinstance(kl, str):
            return ""
        if "å†å²" in kl:
            return "å†"
        if "ç‰©ç†" in kl:
            return "ç‰©"
        return ""


    def parse_subject_requirement(require_text: str, kl: str) -> tuple[str, str]:
        # ===== å…œåº•æ–¹æ¡ˆï¼ˆæ ¸å¿ƒä¿®æ”¹ç‚¹ 1ï¼‰=====
        # åŸå­—æ®µä¸ºç©º / NaN / ç©ºç™½ / nanå­—ç¬¦ä¸² â†’ ç»å¯¹ä¸ç”Ÿæˆé€‰ç§‘ç»“æœ
        if require_text is None:
            return "", ""

        s = str(require_text).strip()

        if s == "" or s.lower() in {"nan", "none"}:
            return "", ""

        # ===== åŸæœ‰é€»è¾‘ï¼Œå®Œå…¨ä¿ç•™ =====
        if "æ–‡ç§‘" in kl or "ç†ç§‘" in kl:
            return "", ""

        text = (
            s.replace(" ", "")
            .replace("ã€€", "")
            .replace("ï¼Œ", "")
            .replace(",", "")
            .replace("ã€", "")

        )
        subject_map = {
            "æ€æƒ³æ”¿æ²»": "æ”¿",
            "æ”¿æ²»": "æ”¿",
            "æ”¿": "æ”¿",

            "ç‰©ç†": "ç‰©",
            "ç‰©": "ç‰©",

            "å†å²": "å†",
            "å†": "å†",

            "åŒ–å­¦": "åŒ–",
            "åŒ–": "åŒ–",

            "ç”Ÿç‰©": "ç”Ÿ",
            "ç”Ÿ": "ç”Ÿ",

            "åœ°ç†": "åœ°",
            "åœ°": "åœ°",
        }

        def extract_all(s: str) -> str:
            res = []
            for k, v in subject_map.items():
                if k in s and v not in res:
                    res.append(v)
            return "".join(res)

        def extract_after_reselect(s: str) -> str:
            if "å†é€‰" in s:
                s = s.split("å†é€‰", 1)[1]
            return extract_all(s)

        if "ä¸é™" in text:
            return "ä¸é™ç§‘ç›®ä¸“ä¸šç»„", ""

        must_keywords = ["å¿…é€‰", "å‡éœ€", "å…¨éƒ¨", "å…¨é€‰", "å‡é¡»", "3ç§‘å¿…é€‰"]
        multi_keywords = ["æˆ–", "/", "ä»»é€‰", "é€‰ä¸€", "è‡³å°‘", "å…¶ä¸­", "ä¹‹ä¸€"]

        is_must = any(k in text for k in must_keywords)
        is_multi = any(k in text for k in multi_keywords)

        req_type = "å¤šé—¨é€‰è€ƒ" if is_multi else "å•ç§‘ã€å¤šç§‘å‡éœ€é€‰è€ƒ"

        # ä¸€å¾‹å…ˆæŠ½å–å…¨éƒ¨ç§‘ç›®
        second = extract_all(text)

        # â­ æ ¸å¿ƒè§„åˆ™ï¼šåªè¦æ˜¯ç‰©ç† / å†å²ç±»ï¼Œå¿…é¡»å‰”é™¤é¦–é€‰
        if "ç‰©ç†" in kl:
            second = second.replace("ç‰©", "")
        elif "å†å²" in kl:
            second = second.replace("å†", "")

        return req_type, second


    # ================= æ ¸å¿ƒåˆå¹¶ =================
    def merge_plan_score(plan_row: pd.Series, score_row: pd.Series) -> dict:
        # ===== å…œåº•æ–¹æ¡ˆï¼ˆæ ¸å¿ƒä¿®æ”¹ç‚¹ 2ï¼ŒåŒä¿é™©ï¼‰=====
        raw_req = plan_row.get("ä¸“ä¸šé€‰ç§‘è¦æ±‚(æ–°é«˜è€ƒä¸“ä¸šçœä»½)", "")

        if not isinstance(raw_req, str) or not raw_req.strip():
            select_req, second_req = "", ""
        else:
            select_req, second_req = parse_subject_requirement(
                raw_req,
                plan_row.get("ç§‘ç±»", "")
            )

        enroll_count = score_row.get("æ‹›ç”Ÿäººæ•°", "")
        if pd.isna(enroll_count) or enroll_count == "":
            enroll_count = plan_row.get("æ‹›ç”Ÿäººæ•°", "")

        level = plan_row.get("å±‚æ¬¡", "")
        if level == "ä¸“ç§‘":
            level = "ä¸“ç§‘(é«˜èŒ)"

        return {
            "å­¦æ ¡åç§°": plan_row.get("å­¦æ ¡", ""),
            "çœä»½": plan_row.get("çœä»½", ""),
            "æ‹›ç”Ÿä¸“ä¸š": plan_row.get("ä¸“ä¸š", ""),
            "ä¸“ä¸šæ–¹å‘ï¼ˆé€‰å¡«ï¼‰": plan_row.get("ä¸“ä¸šæ–¹å‘", ""),
            "ä¸“ä¸šå¤‡æ³¨ï¼ˆé€‰å¡«ï¼‰": plan_row.get("å¤‡æ³¨", ""),
            "å±‚æ¬¡": level,
            "æ‹›ç”Ÿç§‘ç±»": plan_row.get("ç§‘ç±»", ""),
            "æ‹›ç”Ÿæ‰¹æ¬¡": plan_row.get("æ‰¹æ¬¡", ""),
            "æ‹›ç”Ÿç±»å‹ï¼ˆé€‰å¡«ï¼‰": plan_row.get("æ‹›ç”Ÿç±»å‹", ""),
            "æœ€é«˜åˆ†": score_row.get("æœ€é«˜åˆ†", ""),
            "æœ€ä½åˆ†": score_row.get("æœ€ä½åˆ†", ""),
            "å¹³å‡åˆ†": score_row.get("å¹³å‡åˆ†", ""),
            "æœ€ä½åˆ†ä½æ¬¡": score_row.get("æœ€ä½åˆ†ä½æ¬¡", ""),
            "æ‹›ç”Ÿäººæ•°": enroll_count,
            "ä¸“ä¸šç»„ä»£ç ": clean_code_text(plan_row.get("ä¸“ä¸šç»„ä»£ç ", "")),
            "é¦–é€‰ç§‘ç›®": calc_first_subject(plan_row.get("ç§‘ç±»", "")),
            "é€‰ç§‘è¦æ±‚": select_req,
            "æ¬¡é€‰": second_req,
            "ä¸“ä¸šä»£ç ": clean_code_text(plan_row.get("ä¸“ä¸šä»£ç ", "")),
            "æ‹›ç”Ÿä»£ç ": clean_code_text(plan_row.get("æ‹›ç”Ÿä»£ç ", "")),
            "å½•å–äººæ•°": score_row.get("å½•å–äººæ•°", ""),
        }


    def diff_fields(df: pd.DataFrame, fields: list[str]) -> set[str]:
        diffs = set()
        for f in fields:
            if f in df.columns and df[f].nunique(dropna=False) > 1:
                diffs.add(f)
        return diffs


    def clear_cache():
        st.session_state.chosen = {}
        st.session_state.expanded = {}


    # ================= åˆ†æ•°è¡¨æ¨¡æ¿ä¸‹è½½ =================
    st.subheader("ğŸ“¥ åˆ†æ•°è¡¨å¯¼å…¥æ¨¡æ¿")

    template_cols = [
        "å­¦æ ¡", "çœä»½", "ç§‘ç±»", "å±‚æ¬¡", "æ‰¹æ¬¡", "ä¸“ä¸š", "å¤‡æ³¨", "æ‹›ç”Ÿç±»å‹",
        "æœ€é«˜åˆ†", "æœ€ä½åˆ†", "å¹³å‡åˆ†", "æœ€ä½åˆ†ä½æ¬¡", "æ‹›ç”Ÿäººæ•°", "å½•å–äººæ•°"
    ]

    template_df = pd.DataFrame(columns=template_cols)
    buf = BytesIO()
    template_df.to_excel(buf, index=False)
    buf.seek(0)

    st.download_button(
        "â¬‡ ä¸‹è½½ã€åˆ†æ•°è¡¨ã€‘Excelæ¨¡æ¿",
        data=buf,
        file_name="åˆ†æ•°è¡¨å¯¼å…¥æ¨¡æ¿.xlsx"
    )

    # ================= æ•°æ®ä¸Šä¼  =================
    st.subheader("ğŸ“‚ æ•°æ®å¯¼å…¥")

    plan_file = st.file_uploader("ğŸ“˜ ä¸Šä¼ ã€è®¡åˆ’è¡¨ã€‘Excel", type=["xls", "xlsx"])
    score_file = st.file_uploader("ğŸ“™ ä¸Šä¼ ã€åˆ†æ•°è¡¨ã€‘Excel", type=["xls", "xlsx"])

    if plan_file and score_file:
        try:
            plan_df = normalize(pd.read_excel(plan_file))
            score_df = normalize(pd.read_excel(score_file))
            st.success("âœ… æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼Œå¼€å§‹åŒ¹é…...")
            # ... åŒ¹é…ã€ç»Ÿè®¡ã€ä¸‹è½½é€»è¾‘
        except Exception as e:
            st.error(f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
    else:
        st.info("è¯·å…ˆä¸Šä¼ ã€è®¡åˆ’è¡¨ã€‘å’Œã€åˆ†æ•°è¡¨ã€‘")

    # ================= è¯»å–æ•°æ® =================
    plan_df = normalize(pd.read_excel(plan_file))
    score_df = normalize(pd.read_excel(score_file))

    # ================= é€‰ç§‘è¦æ±‚å­—æ®µæ¸…æ´—ï¼ˆä»…æ­¤ä¸€åˆ—ï¼‰ =================
    SUBJECT_COL = "ä¸“ä¸šé€‰ç§‘è¦æ±‚(æ–°é«˜è€ƒä¸“ä¸šçœä»½)"

    if SUBJECT_COL in plan_df.columns:
        plan_df[SUBJECT_COL] = (
            plan_df[SUBJECT_COL]
            .astype(str)
            .str.strip()
            .str.replace(r"^\^", "", regex=True)  # â­ å…³é”®ï¼šå»æ‰å¼€å¤´ ^
            .replace({"nan": "", "None": ""})
        )

    if "ä¸“ä¸šé€‰ç§‘è¦æ±‚(æ–°é«˜è€ƒä¸“ä¸šçœä»½)" not in plan_df.columns:
        plan_df["ä¸“ä¸šé€‰ç§‘è¦æ±‚(æ–°é«˜è€ƒä¸“ä¸šçœä»½)"] = ""

    for k in MATCH_KEYS:
        if k not in plan_df.columns:
            st.error(f"âŒ è®¡åˆ’è¡¨ç¼ºå°‘å­—æ®µï¼š{k}")
            st.stop()
        if k not in score_df.columns:
            st.error(f"âŒ åˆ†æ•°è¡¨ç¼ºå°‘å­—æ®µï¼š{k}")
            st.stop()

    plan_df["_key"] = build_key(plan_df)
    score_df["_key"] = build_key(score_df)
    score_groups = score_df.groupby("_key")

    # ================= åŒ¹é… =================
    unique_rows = []
    duplicate_rows = []
    unmatched_rows = []

    for _, plan_row in plan_df.iterrows():
        key = plan_row["_key"]
        if key not in score_groups.groups:
            unmatched_rows.append(plan_row)
        else:
            group = score_groups.get_group(key)
            if len(group) == 1:
                unique_rows.append(merge_plan_score(plan_row, group.iloc[0]))
            else:
                duplicate_rows.append((plan_row, group))

    # ================= ç»Ÿè®¡ =================
    st.success(
        f"âœ… å”¯ä¸€åŒ¹é…ï¼š{len(unique_rows)} æ¡ ï½œ "
        f"âš  é‡å¤åŒ¹é…ï¼š{len(duplicate_rows)} æ¡ ï½œ "
        f"âŒ æœªåŒ¹é…ï¼š{len(unmatched_rows)} æ¡"
    )

    # ================= Session State =================
    if "chosen" not in st.session_state:
        st.session_state.chosen = {}
    if "expanded" not in st.session_state:
        st.session_state.expanded = {}

    # ================= é‡å¤åŒ¹é… =================
    st.header("âš  é‡å¤åŒ¹é…äººå·¥ç¡®è®¤åŒº")

    total_dup = len(duplicate_rows)
    confirmed = len(st.session_state.chosen)
    progress = 1.0 if total_dup == 0 else confirmed / total_dup

    st.progress(progress)
    st.caption(f"å·²ç¡®è®¤ {confirmed} / {total_dup} æ¡ï¼ˆ{int(progress * 100)}%ï¼‰")

    for i, (plan_row, candidates) in enumerate(duplicate_rows):
        title = (
            f"{i + 1}. "
            f"{plan_row.get('å­¦æ ¡', '')} | "
            f"{plan_row.get('çœä»½', '')} | "
            f"{plan_row.get('ç§‘ç±»', '')} | "
            f"{plan_row.get('æ‰¹æ¬¡', '')} | "
            f"{plan_row.get('ä¸“ä¸š', '')} | "
            f"{safe_text(plan_row.get('å¤‡æ³¨', ''))} | "
            f"{safe_text(plan_row.get('æ‹›ç”Ÿç±»å‹', ''))}"
        )

        with st.expander(title, expanded=False):
            if i in st.session_state.chosen:
                st.success("âœ… å·²é€‰æ‹©å®Œæˆ")
                if st.button("ğŸ” é‡æ–°é€‰æ‹©", key=f"reset_{i}"):
                    del st.session_state.chosen[i]
                    st.rerun()
            else:
                options = [
                    (None, "è¯·é€‰æ‹©å¯¹åº”çš„åˆ†æ•°è®°å½•"),
                    ("NO_SCORE", "ğŸš« æ— å¯¹åº”åˆ†æ•°ï¼ˆä¿ç•™è®¡åˆ’ï¼Œä¸å¡«åˆ†æ•°ï¼‰")
                ]

                diff_cols = diff_fields(candidates, DISPLAY_FIELDS)

                for idx, r in candidates.iterrows():
                    info = []
                    for col in DISPLAY_FIELDS:
                        if col in r and pd.notna(r[col]):
                            if col in diff_cols:
                                info.append(f"ğŸ”´ã€{col}ã€‘{r[col]}")
                            else:
                                info.append(f"{col}:{r[col]}")
                    options.append((idx, " | ".join(info)))

                selected = st.radio(
                    "è¯·é€‰æ‹©å¯¹åº”çš„åˆ†æ•°è®°å½•",
                    options=options,
                    format_func=lambda x: x[1],
                    index=0,
                    key=f"radio_{i}"
                )

                if selected[0] is not None:
                    if st.button("âœ… ç¡®è®¤æœ¬æ¡é€‰æ‹©", key=f"confirm_{i}"):
                        st.session_state.chosen[i] = selected[0]
                        st.rerun()

    # ================= å¯¼å‡º =================
    st.header("ğŸ“¤ å¯¼å‡ºç»“æœ")

    if st.button("ğŸ§¹ æ‰‹åŠ¨æ¸…ç†ç¼“å­˜ï¼ˆé‡æ–°å¼€å§‹åŒ¹é…ï¼‰"):
        clear_cache()
        st.success("ç¼“å­˜å·²æ¸…ç†")
        st.rerun()

    all_chosen = len(st.session_state.chosen) == len(duplicate_rows)

    if st.button("ğŸ“¥ å¯¼å‡ºæœ€ç»ˆå®Œæ•´æ•°æ®", disabled=not all_chosen):
        final_rows = []
        final_rows.extend(unique_rows)

        for i, (plan_row, _) in enumerate(duplicate_rows):
            score_idx = st.session_state.chosen[i]

            if score_idx == "NO_SCORE":
                score_row = {}
            else:
                score_row = score_df.loc[score_idx]

            final_rows.append(merge_plan_score(plan_row, score_row))

        final_df = pd.DataFrame(final_rows)
        unmatched_df = pd.DataFrame(unmatched_rows).drop(columns=["_key"], errors="ignore")

        output = BytesIO()
        with pd.ExcelWriter(output, engine="openpyxl") as writer:
            final_df.to_excel(writer, sheet_name="æœ€ç»ˆå®Œæ•´æ•°æ®", index=False)
            unmatched_df.to_excel(writer, sheet_name="æœªåŒ¹é…æ•°æ®", index=False)

            ws = writer.book["æœ€ç»ˆå®Œæ•´æ•°æ®"]
            for col_idx, col_name in enumerate(final_df.columns, start=1):
                if col_name in TEXT_COLUMNS:
                    col_letter = get_column_letter(col_idx)
                    for row in range(2, ws.max_row + 1):
                        ws[f"{col_letter}{row}"].number_format = "@"

        output.seek(0)

        st.download_button(
            "â¬‡ ä¸‹è½½ Excel",
            data=output,
            file_name=f"åŒ¹é…ç»“æœ_{uuid.uuid4().hex[:6]}.xlsx"
        )

        clear_cache()

    # =====================================================
    # ======================= TAB 5=======================
    # =====================================================
with tab5:
    st.header("ğŸ“Š ä¸“ä¸šåˆ† â†’ ä¸“ä¸šåˆ†-æ‰¹é‡å¯¼å…¥æ¨¡æ¿")
    st.subheader("ğŸ“¥ æ•°æ®ä¸Šä¼ ")

    c1, c2, c3 = st.columns(3)
    with c1:
        prof_file = st.file_uploader(
            "ğŸ“¥ ä¸Šä¼ ã€ä¸“ä¸šåˆ†ï¼ˆæºæ•°æ®ï¼‰ã€‘",
            type=["xls", "xlsx"],
            key="prof"
        )
    with c2:
        school_file = st.file_uploader(
            "ğŸ« å­¦æ ¡å°èŒƒå›´æ•°æ®å¯¼å‡º",
            type=["xls", "xlsx"],
            key="school"
        )
    with c3:
        major_file = st.file_uploader(
            "ğŸ“˜ ä¸“ä¸šä¿¡æ¯è¡¨",
            type=["xls", "xlsx"],
            key="major"
        )

    # ğŸ‘‡ æ³¨æ„ï¼šåˆ¤æ–­ä¸€å®šåœ¨ uploader åé¢
    if not (prof_file and school_file and major_file):
        st.info("è¯·å…ˆä¸Šä¼  3 ä¸ª Excel æ–‡ä»¶")
        st.stop()

    LEVEL_MAP = {
        "1": "æœ¬ç§‘(æ™®é€š)",
        "2": "ä¸“ç§‘(é«˜èŒ)",
        "3": "æœ¬ç§‘(èŒä¸š)"
    }

    GROUP_JOIN_PROVINCE = {
        "æ¹–å—", "ç¦å»º", "å¹¿ä¸œ", "åŒ—äº¬", "é»‘é¾™æ±Ÿ", "å®‰å¾½", "æ±Ÿè¥¿", "å¹¿è¥¿",
        "ç”˜è‚ƒ", "å±±è¥¿", "æ²³å—", "é™•è¥¿", "å®å¤", "å››å·", "äº‘å—", "å†…è’™å¤"
    }

    ONLY_CODE_PROVINCE = {
        "æ¹–åŒ—", "æ±Ÿè‹", "ä¸Šæµ·", "å¤©æ´¥", "æµ·å—", "å‰æ—"
    }

    FINAL_COLUMNS = [
        "å­¦æ ¡åç§°", "çœä»½", "æ‹›ç”Ÿä¸“ä¸š", "ä¸“ä¸šæ–¹å‘ï¼ˆé€‰å¡«ï¼‰", "ä¸“ä¸šå¤‡æ³¨ï¼ˆé€‰å¡«ï¼‰",
        "ä¸€çº§å±‚æ¬¡", "æ‹›ç”Ÿç§‘ç±»", "æ‹›ç”Ÿæ‰¹æ¬¡", "æ‹›ç”Ÿç±»å‹ï¼ˆé€‰å¡«ï¼‰",
        "æœ€é«˜åˆ†", "æœ€ä½åˆ†", "å¹³å‡åˆ†",
        "æœ€ä½åˆ†ä½æ¬¡ï¼ˆé€‰å¡«ï¼‰", "æ‹›ç”Ÿäººæ•°ï¼ˆé€‰å¡«ï¼‰", "æ•°æ®æ¥æº",
        "ä¸“ä¸šç»„ä»£ç ", "é¦–é€‰ç§‘ç›®", "é€‰ç§‘è¦æ±‚", "æ¬¡é€‰ç§‘ç›®",
        "ä¸“ä¸šä»£ç ", "æ‹›ç”Ÿä»£ç ",
        "æœ€ä½åˆ†æ•°åŒºé—´ä½", "æœ€ä½åˆ†æ•°åŒºé—´é«˜",
        "æœ€ä½åˆ†æ•°åŒºé—´ä½æ¬¡ä½", "æœ€ä½åˆ†æ•°åŒºé—´ä½æ¬¡é«˜",
        "å½•å–äººæ•°ï¼ˆé€‰å¡«ï¼‰"
    ]


    # =========================
    # å·¥å…·å‡½æ•°
    # =========================
    def to_float(x):
        if pd.isna(x):
            return None
        s = str(x).strip()
        if s == "" or s == "0":
            return None
        try:
            return float(s)
        except:
            return None


    def score_valid(row):
        max_s = to_float(row["æœ€é«˜åˆ†"])
        min_s = to_float(row["æœ€ä½åˆ†"])
        avg_s = to_float(row["å¹³å‡åˆ†"])

        checks = []

        if max_s is not None and min_s is not None:
            checks.append(max_s >= min_s)
        if max_s is not None and avg_s is not None:
            checks.append(max_s >= avg_s)
        if avg_s is not None and min_s is not None:
            checks.append(avg_s >= min_s)

        return all(checks)


    def convert_subject(x):
        if x == "ç‰©ç†":
            return "ç‰©ç†ç±»", "ç‰©"
        if x == "å†å²":
            return "å†å²ç±»", "å†"
        if x in {"æ–‡ç§‘", "ç†ç§‘", "ç»¼åˆ"}:
            return x, ""
        return x, ""


    def parse_requirement(req):
        if pd.isna(req):
            return "ä¸é™ç§‘ç›®ä¸“ä¸šç»„", ""

        req = str(req).strip()
        if req == "" or req == "ä¸é™":
            return "ä¸é™ç§‘ç›®ä¸“ä¸šç»„", ""

        # æ¬¡é€‰ç§‘ç›®ï¼šåªä¿ç•™ç§‘ç›®æœ¬èº«
        subjects = req.replace("ä¸”", "").replace("/", "")

        # ç‰©/åŒ– â†’ å¤šé—¨é€‰è€ƒ
        if "/" in req:
            return "å¤šé—¨é€‰è€ƒ", subjects

        # ç‰©ä¸”åŒ– / å•ç§‘
        return "å•ç§‘ã€å¤šç§‘å‡éœ€é€‰è€ƒ", subjects


    def build_group_code(row):
        code = row["æ‹›ç”Ÿä»£ç "]
        gid = row["ä¸“ä¸šç»„ç¼–å·"]
        prov = row["çœä»½"]

        if prov in GROUP_JOIN_PROVINCE and pd.notna(gid) and str(gid).strip() != "":
            return f"{code}ï¼ˆ{gid}ï¼‰"
        if prov in ONLY_CODE_PROVINCE:
            return code
        return ""


    def to_excel(df):
        buf = BytesIO()
        df.to_excel(buf, index=False)
        return buf.getvalue()


    # =========================

    # =========================
    # ä¸»é€»è¾‘
    # =========================
    if prof_file and school_file and major_file:

        df = pd.read_excel(prof_file, dtype=str)
        school_df = pd.read_excel(school_file, dtype=str)
        major_df = pd.read_excel(major_file, dtype=str)

        st.subheader("â‘  æ•°æ®æ ¡éªŒ")

        errors = []

        # æ ¡éªŒ1ï¼šå­¦æ ¡åç§°
        bad_school = df[~df["é™¢æ ¡åç§°"].isin(set(school_df["å­¦æ ¡åç§°"]))].copy()
        if not bad_school.empty:
            bad_school["é”™è¯¯åŸå› "] = "å­¦æ ¡åç§°ä¸åœ¨å­¦æ ¡å°èŒƒå›´æ•°æ®ä¸­"
            errors.append(bad_school)

        # æ ¡éªŒ2ï¼šä¸“ä¸š + ä¸€çº§å±‚æ¬¡
        df["ä¸€çº§å±‚æ¬¡"] = df["å±‚æ¬¡"].map(LEVEL_MAP)
        chk = df.merge(
            major_df[["ä¸“ä¸šåç§°", "ä¸€çº§å±‚æ¬¡"]],
            on=["ä¸“ä¸šåç§°", "ä¸€çº§å±‚æ¬¡"],
            how="left",
            indicator=True
        )
        bad_major = chk[chk["_merge"] == "left_only"].copy()
        if not bad_major.empty:
            bad_major["é”™è¯¯åŸå› "] = "ä¸“ä¸šåç§° + ä¸€çº§å±‚æ¬¡ ä¸å­˜åœ¨"
            errors.append(bad_major[df.columns.tolist() + ["é”™è¯¯åŸå› "]])

        # æ ¡éªŒ3ï¼šåˆ†æ•°
        bad_score = df[~df.apply(score_valid, axis=1)].copy()
        if not bad_score.empty:
            bad_score["é”™è¯¯åŸå› "] = "åˆ†æ•°å…³ç³»é”™è¯¯ï¼ˆæœ€é«˜/å¹³å‡/æœ€ä½ï¼‰"
            errors.append(bad_score)

        if errors:
            err_df = pd.concat(errors, ignore_index=True)
            st.error(f"âŒ æ ¡éªŒå¤±è´¥ï¼Œå…± {len(err_df)} æ¡")
            st.dataframe(err_df)
            st.download_button(
                "ğŸ“¥ ä¸‹è½½é”™è¯¯æ˜ç»†",
                data=to_excel(err_df),
                file_name="ä¸“ä¸šåˆ†-æ ¡éªŒé”™è¯¯æ˜ç»†.xlsx"
            )
            st.stop()

        st.success("âœ… æ ¡éªŒé€šè¿‡")

        # =========================
        # å­—æ®µè½¬æ¢
        # =========================
        out = pd.DataFrame()

        out["å­¦æ ¡åç§°"] = df["é™¢æ ¡åç§°"]
        out["çœä»½"] = df["çœä»½"]
        out["æ‹›ç”Ÿä¸“ä¸š"] = df["ä¸“ä¸šåç§°"]
        out["ä¸“ä¸šæ–¹å‘ï¼ˆé€‰å¡«ï¼‰"] = ""
        out["ä¸“ä¸šå¤‡æ³¨ï¼ˆé€‰å¡«ï¼‰"] = df["ä¸“ä¸šå¤‡æ³¨"]
        out["ä¸€çº§å±‚æ¬¡"] = df["ä¸€çº§å±‚æ¬¡"]

        out["æ‹›ç”Ÿç§‘ç±»"], out["é¦–é€‰ç§‘ç›®"] = zip(*df["ç§‘ç±»"].apply(convert_subject))

        out["æ‹›ç”Ÿæ‰¹æ¬¡"] = df["æ‰¹æ¬¡"]
        out["æ‹›ç”Ÿç±»å‹ï¼ˆé€‰å¡«ï¼‰"] = df["æ‹›ç”Ÿç±»å‹"]

        out["æœ€é«˜åˆ†"] = df["æœ€é«˜åˆ†"]
        out["æœ€ä½åˆ†"] = df["æœ€ä½åˆ†"]
        out["å¹³å‡åˆ†"] = df["å¹³å‡åˆ†"]

        out["æœ€ä½åˆ†ä½æ¬¡ï¼ˆé€‰å¡«ï¼‰"] = df["æœ€ä½ä½æ¬¡"]
        out["æ‹›ç”Ÿäººæ•°ï¼ˆé€‰å¡«ï¼‰"] = df["æ‹›ç”Ÿè®¡åˆ’äººæ•°"]

        out["æ•°æ®æ¥æº"] = "å­¦ä¸šæ¡¥"
        out["ä¸“ä¸šç»„ä»£ç "] = df.apply(build_group_code, axis=1)

        out["é€‰ç§‘è¦æ±‚"], out["æ¬¡é€‰ç§‘ç›®"] = zip(*df["æŠ¥è€ƒè¦æ±‚"].apply(parse_requirement))

        out["ä¸“ä¸šä»£ç "] = df["ä¸“ä¸šä»£ç "]
        out["æ‹›ç”Ÿä»£ç "] = df["æ‹›ç”Ÿä»£ç "]

        out["æœ€ä½åˆ†æ•°åŒºé—´ä½"] = ""
        out["æœ€ä½åˆ†æ•°åŒºé—´é«˜"] = ""
        out["æœ€ä½åˆ†æ•°åŒºé—´ä½æ¬¡ä½"] = ""
        out["æœ€ä½åˆ†æ•°åŒºé—´ä½æ¬¡é«˜"] = ""

        out["å½•å–äººæ•°ï¼ˆé€‰å¡«ï¼‰"] = df["å½•å–äººæ•°"]

        # å¼ºåˆ¶å­—æ®µé¡ºåº
        out = out[FINAL_COLUMNS]

        st.dataframe(out.head(20))

        st.download_button(
            "ğŸ“¤ ä¸‹è½½ã€ä¸“ä¸šåˆ†-æ‰¹é‡å¯¼å…¥æ¨¡æ¿ã€‘",
            data=to_excel(out),
            file_name="ä¸“ä¸šåˆ†-æ‰¹é‡å¯¼å…¥æ¨¡æ¿.xlsx"
        )


# ------------------------ Footer ------------------------
st.markdown("---")
st.caption("è¯´æ˜ï¼šå·²é»˜è®¤å¯ç”¨ç»Ÿä¸€è¯·æ±‚é…ç½®ï¼ˆè¶…æ—¶ä¸è¯ä¹¦ç­–ç•¥ï¼‰ã€‚è‹¥éœ€å°† VERIFY_SSL è®¾ä¸º Trueï¼Œè¯·ä¿®æ”¹æ–‡ä»¶é¡¶éƒ¨çš„å¸¸é‡å¹¶é‡å¯ã€‚")