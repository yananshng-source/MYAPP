# app.py
import streamlit as st
import os
import pandas as pd
from io import BytesIO
from PIL import Image, ImageOps
import requests
from urllib.parse import urljoin
from bs4 import BeautifulSoup
import re
from datetime import datetime
import logging
import traceback
from typing import Iterable, Any
import numpy as np
import tkinter as tk
from tkinter import filedialog
# ------------------------ Config ------------------------
st.set_page_config(page_title="ç»¼åˆå¤„ç†å·¥å…·ç®±", layout="wide")
DEFAULT_TIMEOUT = 15
REQUEST_HEADERS = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
VERIFY_SSL = False  # cloud ä¸Šæœ‰äº›ç«™ç‚¹ä¼šè¯ä¹¦é—®é¢˜ï¼Œä¿å®ˆè®¾ä¸º False
MAX_LOG_LINES = 200

# ------------------------ Logging ------------------------
logger = logging.getLogger("ç»¼åˆå¤„ç†å·¥å…·ç®±")
logger.setLevel(logging.INFO)
if not logger.handlers:
    ch = logging.StreamHandler()
    ch.setFormatter(logging.Formatter("%(asctime)s - %(levelname)s - %(message)s"))
    logger.addHandler(ch)

# store recent logs in session state to show in UI
if "recent_logs" not in st.session_state:
    st.session_state.recent_logs = []


def log(msg, level="info"):
    entry = f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - {level.upper()} - {msg}"
    st.session_state.recent_logs.append(entry)
    # cap length
    if len(st.session_state.recent_logs) > MAX_LOG_LINES:
        st.session_state.recent_logs = st.session_state.recent_logs[-MAX_LOG_LINES:]
    if level == "info":
        logger.info(msg)
    elif level == "warning":
        logger.warning(msg)
    elif level == "error":
        logger.error(msg)
    else:
        logger.debug(msg)


# ------------------------ Helpers ------------------------
def progress_iter(it: Iterable[Any], text="å¤„ç†ä¸­...", progress_key=None):
    """
    Generic iterator wrapper that updates a single st.progress bar (main bar).
    It expects an iterable with a determinable length (like list, tuple, DataFrame rows via list()).
    Yields the original items.
    """
    # normalize to list to calculate total reliably (this will hold items in memory)
    # For very large iterables you may replace with a custom strategy.
    items = list(it)
    total = len(items)
    if progress_key is None:
        progress_key = "main_progress"
    progress_bar = st.session_state.get(progress_key)
    if progress_bar is None:
        progress_bar = st.progress(0, text=text)
        st.session_state[progress_key] = progress_bar
    try:
        for idx, item in enumerate(items):
            yield item
            percent = int((idx + 1) / total * 100) if total > 0 else 100
            try:
                progress_bar.progress(percent, text=text)
            except Exception:
                # fallback: ignore progress update error
                pass
        try:
            progress_bar.progress(100, text=text + " âœ… å®Œæˆ")
        except Exception:
            pass
    finally:
        # clear stored progress bar so future calls get a fresh widget
        if progress_key in st.session_state:
            del st.session_state[progress_key]


def safe_requests_get(session: requests.Session, url: str, **kwargs):
    """
    Wrapper around session.get with global headers, timeout, verify options and robust exception handling.
    Returns response or raises.
    """
    try:
        resp = session.get(url, timeout=kwargs.get("timeout", DEFAULT_TIMEOUT),
                           headers=REQUEST_HEADERS, verify=VERIFY_SSL)
        resp.raise_for_status()
        return resp
    except Exception as e:
        raise


def ensure_dir(path):
    os.makedirs(path, exist_ok=True)
    return path


# ------------------------ æ‹›ç”Ÿæ•°æ®å¤„ç†å‡½æ•° ------------------------
def process_admission_data(df_source):
    """
    å¤„ç†æ‹›ç”Ÿæ•°æ®ï¼ŒæŒ‰ç…§æŒ‡å®šè§„åˆ™åˆ†ç»„å¹¶ç”Ÿæˆç»“æœè¡¨æ ¼
    """
    log("å¼€å§‹å¤„ç†æ‹›ç”Ÿæ•°æ®...")

    # æ•°æ®æ¸…æ´—å’Œé¢„å¤„ç† - åªæ›¿æ¢ç‰¹æ®Šå­—ç¬¦ï¼Œä¸å¡«å……ç©ºå€¼
    df_source = df_source.replace({'^': '', '~': ''}, regex=True)

    # å¤„ç†æ•°å€¼å­—æ®µï¼Œä½†ä¸å¡«å……ç©ºå€¼
    numeric_columns = ['æœ€é«˜åˆ†', 'æœ€ä½åˆ†', 'æœ€ä½åˆ†ä½æ¬¡', 'å½•å–äººæ•°']
    for col in numeric_columns:
        if col in df_source.columns:
            df_source[col] = pd.to_numeric(df_source[col], errors='coerce')

    # ç¡®å®šé¦–é€‰ç§‘ç›® - åªé’ˆå¯¹æ–°é«˜è€ƒçœä»½
    def determine_preferred_subject(row):
        col_type = str(row.get('ç§‘ç±»', ''))
        # åªæœ‰å†å²ç±»å’Œç‰©ç†ç±»æ‰æœ‰é¦–é€‰ç§‘ç›®
        if 'å†å²ç±»' in col_type:
            return 'å†å²'
        elif 'ç‰©ç†ç±»' in col_type:
            return 'ç‰©ç†'
        # æ–‡ç§‘ã€ç†ç§‘ã€ç»¼åˆç­‰ä¼ ç»Ÿç§‘ç±»æ²¡æœ‰é¦–é€‰ç§‘ç›®
        return ''

    df_source['é¦–é€‰ç§‘ç›®'] = df_source.apply(determine_preferred_subject, axis=1)

    # ç¡®å®šæ‹›ç”Ÿç±»åˆ«ï¼ˆç§‘ç±»ï¼‰- ä¿®æ­£é€»è¾‘
    def determine_admission_category(row):
        col_type = str(row.get('ç§‘ç±»', ''))
        # æ–°é«˜è€ƒçœä»½ï¼šå†å²ç±»ã€ç‰©ç†ç±»
        if 'å†å²ç±»' in col_type:
            return 'å†å²ç±»'
        elif 'ç‰©ç†ç±»' in col_type:
            return 'ç‰©ç†ç±»'
        # ä¼ ç»Ÿé«˜è€ƒçœä»½ï¼šæ–‡ç§‘ã€ç†ç§‘
        elif 'æ–‡ç§‘' in col_type:
            return 'æ–‡ç§‘'
        elif 'ç†ç§‘' in col_type:
            return 'ç†ç§‘'
        elif 'ç»¼åˆ' in col_type:
            return 'ç»¼åˆ'
        # å…¶ä»–æƒ…å†µä¿æŒåŸæ ·
        return col_type

    df_source['æ‹›ç”Ÿç±»åˆ«'] = df_source.apply(determine_admission_category, axis=1)

    # å¤„ç†å±‚æ¬¡å­—æ®µ - ç¡®ä¿ä¸ä¸ºç©º
    df_source['å±‚æ¬¡'] = df_source.get('å±‚æ¬¡', 'æœ¬ç§‘(æ™®é€š)')
    df_source['å±‚æ¬¡'] = df_source['å±‚æ¬¡'].fillna('æœ¬ç§‘(æ™®é€š)')

    # å¤„ç†æ‹›ç”Ÿç±»å‹ - ç¡®ä¿ä¸ä¸ºç©º
    df_source['æ‹›ç”Ÿç±»å‹'] = df_source.get('æ‹›ç”Ÿç±»å‹', '')
    df_source['æ‹›ç”Ÿç±»å‹'] = df_source['æ‹›ç”Ÿç±»å‹'].fillna('')

    # å¤„ç†ä¸“ä¸šç»„ä»£ç  - ç¡®ä¿ä¸ä¸ºç©º
    df_source['ä¸“ä¸šç»„ä»£ç '] = df_source.get('ä¸“ä¸šç»„ä»£ç ', '')
    df_source['ä¸“ä¸šç»„ä»£ç '] = df_source['ä¸“ä¸šç»„ä»£ç '].fillna('')

    # å¤„ç†å…¶ä»–åˆ†ç»„åˆ— - ç¡®ä¿ä¸ä¸ºç©º
    df_source['çœä»½'] = df_source['çœä»½'].fillna('')
    df_source['æ‰¹æ¬¡'] = df_source['æ‰¹æ¬¡'].fillna('')

    log("æ•°æ®é¢„å¤„ç†å®Œæˆï¼Œå¼€å§‹åˆ†ç»„...")

    # åˆ†ç»„å¤„ç† - æŒ‰ç…§æŒ‡å®šçš„åˆ—åˆ†ç»„
    grouping_columns = ['çœä»½', 'æ‹›ç”Ÿç±»åˆ«', 'æ‰¹æ¬¡', 'å±‚æ¬¡', 'æ‹›ç”Ÿç±»å‹', 'ä¸“ä¸šç»„ä»£ç ']

    log(f"ä½¿ç”¨ä»¥ä¸‹åˆ—è¿›è¡Œåˆ†ç»„: {grouping_columns}")

    # åˆ›å»ºä¸€ä¸ªåˆ—è¡¨æ¥å­˜å‚¨ç»“æœ
    results = []

    # å¯¹æ¯ä¸ªåˆ†ç»„è¿›è¡Œå¤„ç†
    group_count = 0
    for group_key, group_data in df_source.groupby(grouping_columns):
        group_count += 1
        # è§£åŒ…åˆ†ç»„é”®
        çœä»½, æ‹›ç”Ÿç±»åˆ«, æ‰¹æ¬¡, å±‚æ¬¡, æ‹›ç”Ÿç±»å‹, ä¸“ä¸šç»„ä»£ç  = group_key

        # è®¡ç®—ç»„å†…èšåˆå€¼
        æœ€é«˜åˆ† = group_data['æœ€é«˜åˆ†'].max() if 'æœ€é«˜åˆ†' in group_data.columns and not group_data[
            'æœ€é«˜åˆ†'].isna().all() else pd.NA
        æœ€ä½åˆ† = group_data['æœ€ä½åˆ†'].min() if 'æœ€ä½åˆ†' in group_data.columns and not group_data[
            'æœ€ä½åˆ†'].isna().all() else pd.NA

        # æ‰¾åˆ°æœ€ä½åˆ†å¯¹åº”çš„è®°å½•
        æœ€ä½åˆ†ä½æ¬¡ = pd.NA
        æœ€ä½åˆ†ä¸“ä¸šç»„ä»£ç  = ä¸“ä¸šç»„ä»£ç 
        æ•°æ®æ¥æº = ''
        é¦–é€‰ç§‘ç›® = ''
        å­¦æ ¡åç§° = ''

        if pd.notna(æœ€ä½åˆ†) and 'æœ€ä½åˆ†' in group_data.columns:
            min_score_rows = group_data[group_data['æœ€ä½åˆ†'] == æœ€ä½åˆ†]
            if not min_score_rows.empty:
                min_score_row = min_score_rows.iloc[0]
                æœ€ä½åˆ†ä½æ¬¡ = min_score_row.get('æœ€ä½åˆ†ä½æ¬¡', pd.NA)
                æœ€ä½åˆ†ä¸“ä¸šç»„ä»£ç  = min_score_row.get('ä¸“ä¸šç»„ä»£ç ', ä¸“ä¸šç»„ä»£ç )
                æ•°æ®æ¥æº = min_score_row.get('æ•°æ®æ¥æº', '')
                é¦–é€‰ç§‘ç›® = min_score_row.get('é¦–é€‰ç§‘ç›®', '')  # ä½¿ç”¨è®°å½•ä¸­çš„é¦–é€‰ç§‘ç›®
                å­¦æ ¡åç§° = min_score_row.get('å­¦æ ¡', '')

        # å¦‚æœæ²¡æ‰¾åˆ°æœ€ä½åˆ†è®°å½•ï¼Œä½¿ç”¨ç»„å†…ç¬¬ä¸€æ¡è®°å½•
        if not å­¦æ ¡åç§° and len(group_data) > 0:
            first_row = group_data.iloc[0]
            æ•°æ®æ¥æº = first_row.get('æ•°æ®æ¥æº', '')
            é¦–é€‰ç§‘ç›® = first_row.get('é¦–é€‰ç§‘ç›®', '')  # ä½¿ç”¨è®°å½•ä¸­çš„é¦–é€‰ç§‘ç›®
            å­¦æ ¡åç§° = first_row.get('å­¦æ ¡', '')

        # è®¡ç®—å½•å–äººæ•°æ€»å’Œï¼ˆæºæ•°æ®ä¸­æœ‰å½•å–äººæ•°ï¼‰
        å½•å–äººæ•° = group_data['å½•å–äººæ•°'].sum() if 'å½•å–äººæ•°' in group_data.columns and not group_data[
            'å½•å–äººæ•°'].isna().all() else pd.NA

        # æ‹›ç”Ÿäººæ•°ç½®ç©ºï¼ˆæºæ•°æ®ä¸­æ²¡æœ‰ï¼‰
        æ‹›ç”Ÿäººæ•° = pd.NA

        # æ·»åŠ åˆ°ç»“æœåˆ—è¡¨
        results.append({
            'å­¦æ ¡åç§°': å­¦æ ¡åç§°,
            'çœä»½': çœä»½,
            'æ‹›ç”Ÿç±»åˆ«': æ‹›ç”Ÿç±»åˆ«,
            'æ‹›ç”Ÿæ‰¹æ¬¡': æ‰¹æ¬¡,
            'æ‹›ç”Ÿç±»å‹': æ‹›ç”Ÿç±»å‹,
            'æœ€é«˜åˆ†': æœ€é«˜åˆ†,
            'æœ€ä½åˆ†': æœ€ä½åˆ†,
            'æœ€ä½åˆ†ä½æ¬¡': æœ€ä½åˆ†ä½æ¬¡,
            'å½•å–äººæ•°': å½•å–äººæ•°,
            'æ‹›ç”Ÿäººæ•°': æ‹›ç”Ÿäººæ•°,  # ç½®ç©º
            'æ•°æ®æ¥æº': æ•°æ®æ¥æº,
            'ä¸“ä¸šç»„ä»£ç ': æœ€ä½åˆ†ä¸“ä¸šç»„ä»£ç ,  # ä½¿ç”¨æœ€ä½åˆ†å¯¹åº”çš„ä¸“ä¸šç»„ä»£ç 
            'é¦–é€‰ç§‘ç›®': é¦–é€‰ç§‘ç›®,  # åªæœ‰å†å²ç±»/ç‰©ç†ç±»æ‰æœ‰å€¼ï¼Œå…¶ä»–ä¸ºç©º
            'é™¢æ ¡æ‹›ç”Ÿä»£ç ': ''  # ä¿æŒç©ºå€¼
        })

    log(f"åˆ†ç»„å¤„ç†å®Œæˆï¼Œå…± {group_count} ä¸ªåˆ†ç»„")

    # åˆ›å»ºç»“æœDataFrame
    result_df = pd.DataFrame(results)

    log(f"åˆ†ç»„åå…±æœ‰ {len(result_df)} ç»„æ•°æ®")

    # ç¡®ä¿æ•°å€¼å­—æ®µä¿æŒæ­£ç¡®çš„æ•°æ®ç±»å‹
    numeric_columns = ['æœ€é«˜åˆ†', 'æœ€ä½åˆ†', 'æœ€ä½åˆ†ä½æ¬¡', 'å½•å–äººæ•°']
    for col in numeric_columns:
        if col in result_df.columns:
            result_df[col] = pd.to_numeric(result_df[col], errors='coerce')

    log(f"å¤„ç†å®Œæˆï¼Œå…±ç”Ÿæˆ {len(result_df)} è¡Œè®°å½•")

    return result_df


# ------------------------ Core functions ------------------------
def scrape_table(url_list, group_cols):
    """
    Scrape tables from provided URLs using pandas.read_html.
    Returns BytesIO of an Excel file (multiple sheets) or None if nothing found.
    """
    session = requests.Session()
    sheet_data = {}
    all_data = []
    errors = []

    enumerated = list(enumerate(url_list, start=1))
    for idx, url in progress_iter(enumerated, text="æŠ“å–ç½‘é¡µè¡¨æ ¼ä¸­"):
        try:
            _, page_url = (idx, url)
            resp = safe_requests_get(session, page_url)
            text = resp.text
            try:
                dfs = pd.read_html(text)
            except Exception as e:
                msg = f"read_html å¤±è´¥: {page_url} -> {e}"
                log(msg, level="warning")
                errors.append(msg)
                # continue to next url; don't drop entire page because maybe other pages ok
                continue

            for i, df in enumerate(dfs):
                name = f"ç½‘é¡µ{idx}_è¡¨{i + 1}"
                sheet_data[name] = df
                all_data.append(df)
                log(f"æŠ“å–åˆ°è¡¨æ ¼: {name} ({len(df)} è¡Œ)")
        except Exception as e:
            log(f"æŠ“å– URL å¤±è´¥: {url} -> {repr(e)}", level="warning")
            errors.append(f"{url} -> {repr(e)}")
            continue

    if sheet_data:
        output = BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            for name, df in sheet_data.items():
                safe_name = name[:31]
                df.to_excel(writer, sheet_name=safe_name, index=False)
            if all_data:
                try:
                    pd.concat(all_data, ignore_index=True).to_excel(writer, sheet_name="æ±‡æ€»", index=False)
                except Exception as e:
                    log(f"åˆå¹¶æ±‡æ€»è¡¨å¤±è´¥: {e}", level="warning")
        output.seek(0)
        return output
    else:
        log("æœªæŠ“å–åˆ°ä»»ä½•è¡¨æ ¼ã€‚", level="warning")
        return None


def download_images_from_urls(url_list, output_dir=None):
    """
    ä»æ¯ä¸ªé¡µé¢æŠ“å– <img> å¹¶ä¸‹è½½ã€‚
    è¿”å› (output_dir, downloaded_file_paths)
    """
    if output_dir is None:
        output_dir = os.path.join(os.path.expanduser("~"), "Desktop", "downloaded_images")
    ensure_dir(output_dir)
    session = requests.Session()
    session.headers.update(REQUEST_HEADERS)
    downloaded_files = []
    errors = []

    enumerated = list(enumerate(url_list, start=1))
    for idx, url in progress_iter(enumerated, text="ä¸‹è½½ç½‘é¡µå›¾ç‰‡ä¸­"):
        try:
            _, page_url = (idx, url)
            resp = safe_requests_get(session, page_url)
            soup = BeautifulSoup(resp.content, "html.parser")
            title_tag = soup.find("title")
            title = title_tag.string.strip() if title_tag and title_tag.string else f"ç½‘é¡µ{idx}"
            safe_title = "".join([c if c not in r'\/:*?"<>|' else "_" for c in title])
            imgs = soup.find_all("img")
            if not imgs:
                log(f"{page_url} - æœªæ‰¾åˆ° img æ ‡ç­¾", level="info")
            for i, img_tag in enumerate(imgs, start=1):
                src = img_tag.get("src") or img_tag.get("data-src") or img_tag.get("data-original")
                if not src:
                    continue
                full_url = urljoin(page_url, src.strip())
                try:
                    resp_img = safe_requests_get(session, full_url)
                    ext = os.path.splitext(full_url)[1]
                    if not ext or len(ext) > 6:
                        ext = ".jpg"
                    fname = f"{safe_title}_{i}{ext}"
                    fpath = os.path.join(output_dir, fname)
                    with open(fpath, "wb") as f:
                        f.write(resp_img.content)
                    downloaded_files.append(fpath)
                except Exception as e:
                    errors.append(f"å›¾ç‰‡ä¸‹è½½å¤±è´¥: {full_url} -> {repr(e)}")
                    log(f"å›¾ç‰‡ä¸‹è½½å¤±è´¥: {full_url} -> {e}", level="warning")
                    continue
        except Exception as e:
            log(f"é¡µé¢è¯·æ±‚å¤±è´¥: {url} -> {e}", level="warning")
            errors.append(f"{url} -> {repr(e)}")
            continue
    return output_dir, downloaded_files, errors


def crop_images_only(folder_path, x_center, y_center, crop_width, crop_height):
    output_folder = os.path.join(os.path.expanduser("~"), "Desktop", "crop_results")
    ensure_dir(output_folder)
    img_exts = (".png", ".jpg", ".jpeg", ".bmp", ".tif", ".tiff")
    filenames = [f for f in os.listdir(folder_path) if f.lower().endswith(img_exts)]
    for filename in progress_iter(filenames, text="è£å‰ªå›¾ç‰‡ä¸­"):
        try:
            image_path = os.path.join(folder_path, filename)
            img = Image.open(image_path).convert("RGB")
            width, height = img.size
            left = max(0, int(x_center - crop_width // 2))
            right = min(width, int(x_center + crop_width // 2))
            top = max(0, int(y_center - crop_height // 2))
            bottom = min(height, int(y_center + crop_height // 2))
            crop_img = img.crop((left, top, right, bottom))
            # æ”¾å¤§äºŒå€ç”¨äºåç»­è¯†åˆ«/æŸ¥çœ‹
            crop_img = crop_img.resize((crop_img.width * 2, crop_img.height * 2), Image.LANCZOS)
            bw = ImageOps.grayscale(crop_img)
            save_path = os.path.join(output_folder, f"crop_{filename}")
            bw.save(save_path)
            log(f"è£å‰ªå¹¶ä¿å­˜: {save_path}")
        except Exception as e:
            log(f"è£å‰ªå¤±è´¥: {filename} -> {e}", level="warning")
            continue
    return output_folder


# ------------------------ é€‰ç§‘è½¬æ¢ä¸æ—¥æœŸå¤„ç† helpers ------------------------
def convert_selection_requirements(df):
    subject_mapping = {'ç‰©ç†': 'ç‰©', 'åŒ–å­¦': 'åŒ–', 'ç”Ÿç‰©': 'ç”Ÿ', 'å†å²': 'å†', 'åœ°ç†': 'åœ°', 'æ”¿æ²»': 'æ”¿',
                       'æ€æƒ³æ”¿æ²»': 'æ”¿'}
    df_new = df.copy()
    df_new['é¦–é€‰ç§‘ç›®'] = ''
    df_new['é€‰ç§‘è¦æ±‚ç±»å‹'] = ''
    df_new['æ¬¡é€‰'] = ''

    # iterate rows - we selected "row" granular progress behavior
    total_rows = len(df)
    for idx, row in progress_iter(list(df.iterrows()), text="é€‰ç§‘è½¬æ¢ä¸­"):
        try:
            i, r = row
            text = str(r.get('é€‰ç§‘è¦æ±‚', '')).strip()
            cat = str(r.get('æ‹›ç”Ÿç§‘ç±»', '')).strip()
            subjects = [subject_mapping.get(s, s) for s in
                        re.findall(r'ç‰©ç†|åŒ–å­¦|ç”Ÿç‰©|å†å²|åœ°ç†|æ”¿æ²»|æ€æƒ³æ”¿æ²»', text)]
            first = ''
            for s_full, s_short in subject_mapping.items():
                if f'é¦–é€‰{s_full}' in text:
                    first = s_short
            if not first:
                if 'ç‰©ç†' in cat:
                    first = 'ç‰©'
                elif 'å†å²' in cat:
                    first = 'å†'
            remaining = [s for s in subjects if s != first]
            second = ''.join(remaining)
            if 'ä¸é™' in text:
                req_type = 'ä¸é™ç§‘ç›®ä¸“ä¸šç»„'
            elif len(remaining) >= 1:
                req_type = 'å¤šé—¨é€‰è€ƒ'
            else:
                req_type = 'å•ç§‘ã€å¤šç§‘å‡éœ€é€‰è€ƒ'
            df_new.at[i, 'é¦–é€‰ç§‘ç›®'] = first
            df_new.at[i, 'æ¬¡é€‰'] = second
            df_new.at[i, 'é€‰ç§‘è¦æ±‚ç±»å‹'] = req_type
        except Exception as e:
            log(f"é€‰ç§‘è¡Œå¤„ç†å¤±è´¥: idx={i} -> {e}", level="warning")
            continue
    return df_new


def safe_parse_datetime(datetime_str, year):
    if pd.isna(datetime_str):
        return None
    datetime_str = str(datetime_str).strip()
    if not re.search(r'(^|\D)\d{4}(\D|$)', datetime_str):
        datetime_str = f"{year}å¹´{datetime_str}"
    patterns = [(r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥(\d{1,2}):(\d{1,2})', '%Yå¹´%mæœˆ%dæ—¥%H:%M'),
                (r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥', '%Yå¹´%mæœˆ%dæ—¥'),
                (r'(\d{4})-(\d{1,2})-(\d{1,2})', '%Y-%m-%d'),
                (r'(\d{4})/(\d{1,2})/(\d{1,2})', '%Y/%m/%d')]
    for pattern, fmt in patterns:
        try:
            dt = datetime.strptime(datetime_str, fmt)
            return dt
        except Exception:
            continue
    return None


def process_date_range(date_str, year):
    if pd.isna(date_str):
        return date_str, "", ""
    date_str = str(date_str).strip()
    if '-' in date_str:
        start_str, end_str = date_str.split('-', 1)
        start_dt = safe_parse_datetime(start_str, year)
        end_dt = safe_parse_datetime(end_str, year)
        if not start_dt or not end_dt:
            return date_str, "æ ¼å¼é”™è¯¯", "æ ¼å¼é”™è¯¯"
        if ':' not in start_str:
            start_dt = start_dt.replace(hour=0, minute=0, second=0)
        if ':' not in end_str:
            end_dt = end_dt.replace(hour=23, minute=59, second=59)
        if end_dt < start_dt:
            # assume cross-year, å°è¯•å°†ç»“æŸå¹´è®¾åˆ°ä¸‹ä¸€å¹´
            try:
                end_dt = end_dt.replace(year=start_dt.year + 1)
            except Exception:
                pass
        return date_str, start_dt.strftime('%Y-%m-%d %H:%M:%S'), end_dt.strftime('%Y-%m-%d %H:%M:%S')
    else:
        dt = safe_parse_datetime(date_str, year)
        if not dt:
            return date_str, "æ ¼å¼é”™è¯¯", "æ ¼å¼é”™è¯¯"
        start_dt = dt.replace(hour=0, minute=0, second=0) if ':' not in date_str else dt
        end_dt = dt.replace(hour=23, minute=59, second=59) if ':' not in date_str else dt
        return date_str, start_dt.strftime('%Y-%m-%d %H:%M:%S'), end_dt.strftime('%Y-%m-%d %H:%M:%S')


# ------------------------ Streamlit UI ------------------------
st.title("ğŸ§° ç»¼åˆå¤„ç†å·¥å…·ç®± - å®Œæ•´ç‰ˆï¼ˆå¸¦è¿›åº¦æ¡ & æ—¥å¿—ï¼‰")
tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs([
    "ç½‘é¡µè¡¨æ ¼æŠ“å–",
    "ç½‘é¡µå›¾ç‰‡ä¸‹è½½",
    "å›¾ç‰‡è£å‰ª",
    "é«˜æ ¡é€‰ç§‘è½¬æ¢",
    "Excelæ—¥æœŸå¤„ç†",
    "æ‹›ç”Ÿæ•°æ®å¤„ç†"
])

# side: logs
with st.sidebar.expander("è¿è¡Œæ—¥å¿—ï¼ˆæœ€æ–°ï¼‰", expanded=True):
    for line in st.session_state.recent_logs[-200:]:
        st.text(line)

# ------------------------ Tab 1: ç½‘é¡µè¡¨æ ¼æŠ“å– ------------------------
with tab1:
    st.subheader("ç½‘é¡µè¡¨æ ¼æŠ“å–")
    urls_text = st.text_area("è¾“å…¥ç½‘é¡µURLåˆ—è¡¨ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰", height=160)
    group_cols = st.text_input("åˆ†ç»„åˆ—ï¼ˆé€—å·åˆ†éš”ï¼Œå¯é€‰ï¼‰")
    col1, col2 = st.columns([1, 3])
    with col1:
        if st.button("æŠ“å–è¡¨æ ¼", key="scrape"):
            url_list = [u.strip() for u in urls_text.splitlines() if u.strip()]
            if not url_list:
                st.warning("è¯·å…ˆè¾“å…¥æœ‰æ•ˆURLåˆ—è¡¨")
            else:
                try:
                    # start the job
                    output = scrape_table(url_list, group_cols)
                    if output:
                        st.success("æŠ“å–å®Œæˆï¼Œå‡†å¤‡ä¸‹è½½")
                        st.download_button("ä¸‹è½½æŠ“å–è¡¨æ ¼", data=output.getvalue(), file_name="ç½‘é¡µæŠ“å–.xlsx")
                    else:
                        st.warning("æœªæŠ“å–åˆ°è¡¨æ ¼æ•°æ®")
                except Exception as e:
                    log(f"æŠ“å–è¡¨æ ¼æ€»æµç¨‹å¤±è´¥: {e}", level="error")
                    st.error("æŠ“å–è¡¨æ ¼å‡ºé”™ï¼Œè¯¦æƒ…è§æ—¥å¿—")

# ------------------------ Tab 2: ç½‘é¡µå›¾ç‰‡ä¸‹è½½ ------------------------


with tab2:
    st.subheader("ç½‘é¡µå›¾ç‰‡ä¸‹è½½")
    urls_text2 = st.text_area("è¾“å…¥ç½‘é¡µURLåˆ—è¡¨ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰", height=160, key="img_urls")

    col_dir, col_btn = st.columns([3,1])
    with col_dir:
        outdir_input = st.text_input("è¾“å‡ºæ–‡ä»¶å¤¹ï¼ˆå¯é€‰ï¼Œç•™ç©ºåˆ™ä¿å­˜åˆ°æ¡Œé¢é»˜è®¤æ–‡ä»¶å¤¹ï¼‰", value="", key="img_outdir")
    with col_btn:
        if st.button("é€‰æ‹©æ–‡ä»¶å¤¹"):
            try:
                # ä»…åœ¨æœ¬åœ°è¿è¡Œæœ‰æ•ˆï¼Œäº‘ç«¯ä¸æ”¯æŒå¼¹çª—
                root = tk.Tk()
                root.withdraw()
                folder_selected = filedialog.askdirectory()
                if folder_selected:
                    outdir_input = folder_selected
                    st.session_state["img_outdir"] = outdir_input
                    st.success(f"å·²é€‰æ‹©æ–‡ä»¶å¤¹: {outdir_input}")
            except Exception as e:
                st.warning(f"é€‰æ‹©æ–‡ä»¶å¤¹å¤±è´¥: {e}")

    col1, col2 = st.columns([1, 3])
    with col1:
        if st.button("ä¸‹è½½å›¾ç‰‡", key="img_download"):
            url_list = [u.strip() for u in urls_text2.splitlines() if u.strip()]
            if not url_list:
                st.warning("è¯·å…ˆè¾“å…¥æœ‰æ•ˆURLåˆ—è¡¨")
            else:
                # å¦‚æœç”¨æˆ·æ²¡æœ‰è¾“å…¥æˆ–è·¯å¾„ä¸å­˜åœ¨ï¼Œä½¿ç”¨æ¡Œé¢é»˜è®¤æ–‡ä»¶å¤¹å¹¶ç¡®ä¿åˆ›å»º
                if not outdir_input.strip():
                    target_dir = os.path.join(os.path.expanduser("~"), "Desktop", "downloaded_images")
                else:
                    target_dir = os.path.abspath(outdir_input.strip())

                try:
                    # ç¡®ä¿æ–‡ä»¶å¤¹å­˜åœ¨
                    os.makedirs(target_dir, exist_ok=True)

                    # ä¸‹è½½å›¾ç‰‡
                    output_dir, files, errors = download_images_from_urls(url_list, target_dir)
                    st.success(f"å®Œæˆï¼å…±ä¸‹è½½ {len(files)} å¼ å›¾ç‰‡ï¼Œä¿å­˜åˆ°: {output_dir}")

                    if files:
                        # æ˜¾ç¤ºå‰8å¼ ç¼©ç•¥å›¾
                        preview = files[:8]
                        cols = st.columns(len(preview))
                        for c, fp in zip(cols, preview):
                            try:
                                c.image(fp, caption=os.path.basename(fp), use_column_width=True)
                            except Exception:
                                c.write(os.path.basename(fp))

                    if errors:
                        st.warning(f"æœ‰ {len(errors)} ä¸ªé”™è¯¯ï¼ˆè§æ—¥å¿—ï¼‰")
                except Exception as e:
                    log(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥: {e}\n{traceback.format_exc()}", level="error")
                    st.error(f"ä¸‹è½½å›¾ç‰‡å‡ºé”™ï¼Œè¯¦æƒ…è§æ—¥å¿—ã€‚ç›®æ ‡è·¯å¾„: {target_dir}")


# ------------------------ Tab 3: å›¾ç‰‡è£å‰ª ------------------------
with tab3:
    st.subheader("å›¾ç‰‡è£å‰ªï¼ˆä»…è£å‰ªä¿å­˜ï¼‰")
    folder_path = st.text_input("å›¾ç‰‡æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆç»å¯¹è·¯å¾„ï¼‰", key="img_folder")
    x_center = st.number_input("é¡µç ä¸­å¿ƒX", value=788, key="x_center")
    y_center = st.number_input("é¡µç ä¸­å¿ƒY", value=1955, key="y_center")
    crop_w = st.number_input("è£å‰ªå®½åº¦(px)", value=200, key="crop_w")
    crop_h = st.number_input("è£å‰ªé«˜åº¦(px)", value=50, key="crop_h")
    if st.button("è£å‰ªå›¾ç‰‡", key="crop_btn"):
        if not folder_path or not os.path.exists(folder_path):
            st.warning("è¯·æä¾›æœ‰æ•ˆå›¾ç‰‡æ–‡ä»¶å¤¹è·¯å¾„")
        else:
            try:
                output_folder = crop_images_only(folder_path, x_center, y_center, crop_w, crop_h)
                st.success(f"å®Œæˆï¼è£å‰ªç»“æœå·²ä¿å­˜åˆ°ï¼š{output_folder}")
            except Exception as e:
                log(f"è£å‰ªå¤±è´¥: {e}\n{traceback.format_exc()}", level="error")
                st.error("è£å‰ªå¼‚å¸¸ï¼Œè¯¦æƒ…è§æ—¥å¿—")

# ------------------------ Tab 4: é«˜æ ¡é€‰ç§‘è½¬æ¢ ------------------------
with tab4:
    st.subheader("é«˜æ ¡é€‰ç§‘è½¬æ¢")
    uploaded_file = st.file_uploader("ä¸Šä¼ Excelæ–‡ä»¶", type=["xlsx", "xls"], key="sel_excel")
    if uploaded_file:
        try:
            df = pd.read_excel(uploaded_file)
            st.write("åŸå§‹æ•°æ®é¢„è§ˆ", df.head())
            if st.button("è½¬æ¢é€‰ç§‘", key="sel_btn"):
                try:
                    df_result = convert_selection_requirements(df)
                    st.write("è½¬æ¢ç»“æœé¢„è§ˆ", df_result.head())
                    towrite = BytesIO()
                    df_result.to_excel(towrite, index=False)
                    towrite.seek(0)
                    st.download_button("ä¸‹è½½è½¬æ¢ç»“æœExcel", data=towrite.getvalue(), file_name="é€‰ç§‘è½¬æ¢ç»“æœ.xlsx")
                    st.success("é€‰ç§‘è½¬æ¢å®Œæˆ")
                except Exception as e:
                    log(f"é€‰ç§‘è½¬æ¢å¤±è´¥: {e}\n{traceback.format_exc()}", level="error")
                    st.error("é€‰ç§‘è½¬æ¢å‡ºé”™ï¼Œè¯¦æƒ…è§æ—¥å¿—")
        except Exception as e:
            log(f"è¯»å–ä¸Šä¼ æ–‡ä»¶å¤±è´¥: {e}", level="error")
            st.error("æ— æ³•è¯»å–ä¸Šä¼ çš„ Excel æ–‡ä»¶")

# ------------------------ Tab 5: Excelæ—¥æœŸå¤„ç† ------------------------
with tab5:
    st.subheader("Excelæ—¥æœŸå¤„ç†")
    uploaded_file2 = st.file_uploader("ä¸Šä¼ Excelæ–‡ä»¶", type=["xlsx", "xls"], key="date_excel")
    year = st.number_input("å¹´ä»½ï¼ˆç”¨äºè¡¥å…¨ï¼‰", value=datetime.now().year, key="date_year")
    date_col = st.text_input("æ—¥æœŸåˆ—å", value="æ—¥æœŸ", key="date_col")

    if uploaded_file2:
        try:
            df2 = pd.read_excel(uploaded_file2)
            st.write("åŸå§‹æ•°æ®é¢„è§ˆ", df2.head())
            if st.button("å¤„ç†æ—¥æœŸ", key="date_btn"):
                try:
                    start_times = []
                    end_times = []
                    originals = []
                    # row-by-row processing (you selected 'row' granular mode)
                    for d in progress_iter(list(df2[date_col]), text="æ—¥æœŸå¤„ç†ä¸­"):
                        orig, start, end = process_date_range(d, int(year))
                        originals.append(orig)
                        start_times.append(start)
                        end_times.append(end)
                    df2_result = df2.copy()
                    insert_at = df2_result.columns.get_loc(date_col) + 1
                    df2_result.insert(insert_at, 'å¼€å§‹æ—¶é—´', start_times)
                    df2_result.insert(insert_at + 1, 'ç»“æŸæ—¶é—´', end_times)
                    st.write("å¤„ç†ç»“æœé¢„è§ˆ", df2_result.head())
                    towrite2 = BytesIO()
                    df2_result.to_excel(towrite2, index=False)
                    towrite2.seek(0)
                    st.download_button("ä¸‹è½½æ—¥æœŸå¤„ç†ç»“æœExcel", data=towrite2.getvalue(), file_name="æ—¥æœŸå¤„ç†ç»“æœ.xlsx")
                    st.success("æ—¥æœŸå¤„ç†å®Œæˆ")
                except Exception as e:
                    log(f"æ—¥æœŸå¤„ç†å¤±è´¥: {e}\n{traceback.format_exc()}", level="error")
                    st.error("æ—¥æœŸå¤„ç†å‡ºé”™ï¼Œè¯¦æƒ…è§æ—¥å¿—")
        except Exception as e:
            log(f"è¯»å–ä¸Šä¼ æ–‡ä»¶å¤±è´¥: {e}", level="error")
            st.error("æ— æ³•è¯»å–ä¸Šä¼ çš„ Excel æ–‡ä»¶")

# ------------------------ Tab 6: æ‹›ç”Ÿæ•°æ®å¤„ç† ------------------------
with tab6:
    st.subheader("ğŸ“ æ‹›ç”Ÿæ•°æ®å¤„ç†")
    st.markdown("""
    æœ¬å·¥å…·æŒ‰ç…§ä»¥ä¸‹è§„åˆ™å¤„ç†æ‹›ç”Ÿæ•°æ®ï¼š
    - **åˆ†ç»„è§„åˆ™**ï¼šçœä»½ã€ç§‘ç±»ã€æ‰¹æ¬¡ã€å±‚æ¬¡ã€æ‹›ç”Ÿç±»å‹ã€ä¸“ä¸šç»„ä»£ç 
    - **èšåˆè§„åˆ™**ï¼š
      - æœ€é«˜åˆ† = ç»„å†…æœ€é«˜åˆ†çš„æœ€å¤§å€¼
      - æœ€ä½åˆ† = ç»„å†…æœ€ä½åˆ†çš„æœ€å°å€¼  
      - æœ€ä½åˆ†ä½æ¬¡ = æœ€ä½åˆ†å¯¹åº”çš„ä½æ¬¡
      - å½•å–äººæ•° = ç»„å†…å½•å–äººæ•°æ€»å’Œ
      - æ‹›ç”Ÿäººæ•° = ç©ºå€¼ï¼ˆæºæ•°æ®ä¸­æ²¡æœ‰ï¼‰
      - ä¸“ä¸šç»„ä»£ç  = æœ€ä½åˆ†å¯¹åº”çš„ä¸“ä¸šç»„ä»£ç 
      - é¦–é€‰ç§‘ç›® = åªæœ‰å†å²ç±»/ç‰©ç†ç±»æ‰æœ‰å€¼ï¼Œå…¶ä»–ä¸ºç©º
    """)

    # æ–‡ä»¶ä¸Šä¼ 
    uploaded_file_admission = st.file_uploader(
        "ä¸Šä¼ æ‹›ç”Ÿæ•°æ®Excelæ–‡ä»¶",
        type=['xlsx'],
        help="è¯·ä¸Šä¼ åŒ…å«æ‹›ç”Ÿæ•°æ®çš„Excelæ–‡ä»¶",
        key="admission_excel"
    )

    if uploaded_file_admission is not None:
        try:
            # è¯»å–ä¸Šä¼ çš„æ–‡ä»¶
            df_source = pd.read_excel(uploaded_file_admission)

            # æ˜¾ç¤ºæºæ•°æ®é¢„è§ˆ
            st.subheader("ğŸ“Š æºæ•°æ®é¢„è§ˆ")
            st.dataframe(df_source.head(10), use_container_width=True)
            st.write(f"æºæ•°æ®å½¢çŠ¶: {df_source.shape}")

            # å¤„ç†æŒ‰é’®
            if st.button("ğŸš€ å¼€å§‹å¤„ç†æ‹›ç”Ÿæ•°æ®", type="primary", key="admission_btn"):
                with st.spinner("æ­£åœ¨å¤„ç†æ‹›ç”Ÿæ•°æ®ï¼Œè¯·ç¨å€™..."):
                    result_df = process_admission_data(df_source)

                if len(result_df) == 0:
                    st.error("è­¦å‘Šï¼šæ²¡æœ‰ç”Ÿæˆä»»ä½•æ•°æ®ï¼Œè¯·æ£€æŸ¥æºæ•°æ®æ–‡ä»¶")
                    st.stop()

                # æ˜¾ç¤ºå¤„ç†ç»“æœ
                st.subheader("âœ… å¤„ç†ç»“æœ")

                # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("å­¦æ ¡æ•°é‡", result_df['å­¦æ ¡åç§°'].nunique())
                with col2:
                    st.metric("çœä»½æ•°é‡", result_df['çœä»½'].nunique())
                with col3:
                    st.metric("æ€»è®°å½•æ•°", len(result_df))
                with col4:
                    total_enrollment = result_df['å½•å–äººæ•°'].sum() if 'å½•å–äººæ•°' in result_df.columns and not result_df[
                        'å½•å–äººæ•°'].isna().all() else 0
                    st.metric("æ€»å½•å–äººæ•°", int(total_enrollment))

                # æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
                st.dataframe(result_df, use_container_width=True)

                # æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡
                st.subheader("ğŸ“ˆ è¯¦ç»†ç»Ÿè®¡")

                col1, col2 = st.columns(2)

                with col1:
                    st.write("**æ‹›ç”Ÿç±»åˆ«åˆ†å¸ƒ**")
                    st.write(result_df['æ‹›ç”Ÿç±»åˆ«'].value_counts())

                    st.write("**é¦–é€‰ç§‘ç›®åˆ†å¸ƒ**")
                    preferred_subject_dist = result_df['é¦–é€‰ç§‘ç›®'].value_counts()
                    if '' in preferred_subject_dist:
                        preferred_subject_dist['ç©ºå€¼'] = preferred_subject_dist.pop('')
                    st.write(preferred_subject_dist)

                with col2:
                    if 'æœ€é«˜åˆ†' in result_df.columns:
                        valid_max_scores = result_df['æœ€é«˜åˆ†'].dropna()
                        if len(valid_max_scores) > 0:
                            st.write("**åˆ†æ•°èŒƒå›´**")
                            st.write(f"æœ€é«˜åˆ†: {valid_max_scores.min():.1f} - {valid_max_scores.max():.1f}")

                    if 'æœ€ä½åˆ†' in result_df.columns:
                        valid_min_scores = result_df['æœ€ä½åˆ†'].dropna()
                        if len(valid_min_scores) > 0:
                            st.write(f"æœ€ä½åˆ†: {valid_min_scores.min():.1f} - {valid_min_scores.max():.1f}")

                # ä¸‹è½½åŠŸèƒ½
                st.subheader("ğŸ“¥ ä¸‹è½½å¤„ç†ç»“æœ")

                # å°†DataFrameè½¬æ¢ä¸ºExcelå­—èŠ‚æµ
                output = BytesIO()
                with pd.ExcelWriter(output, engine='openpyxl') as writer:
                    result_df.to_excel(writer, index=False, sheet_name='å¤„ç†ç»“æœ')

                processed_data = output.getvalue()

                st.download_button(
                    label="ğŸ“¥ ä¸‹è½½å¤„ç†åçš„Excelæ–‡ä»¶",
                    data=processed_data,
                    file_name="åˆ†ç»„å¤„ç†åçš„æ‹›ç”Ÿæ•°æ®.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    key="download_admission"
                )

        except Exception as e:
            st.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            st.info("è¯·æ£€æŸ¥ä¸Šä¼ çš„æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®")

# ------------------------ Footer ------------------------
st.markdown("---")
st.caption("è¯´æ˜ï¼šå·²é»˜è®¤å¯ç”¨ç»Ÿä¸€è¯·æ±‚é…ç½®ï¼ˆè¶…æ—¶ä¸è¯ä¹¦ç­–ç•¥ï¼‰ã€‚è‹¥éœ€å°† VERIFY_SSL è®¾ä¸º Trueï¼Œè¯·ä¿®æ”¹æ–‡ä»¶é¡¶éƒ¨çš„å¸¸é‡å¹¶é‡å¯ã€‚") 