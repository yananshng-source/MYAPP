import streamlit as st
import pandas as pd
from io import BytesIO, StringIO
import requests
from bs4 import BeautifulSoup

# æ¸…ç† DataFrame
def clean_df(df, group_cols=None):
    if group_cols:
        fill_cols = [col for col in group_cols if col in df.columns]
        if fill_cols:
            df[fill_cols] = df[fill_cols].ffill()
    df = df.apply(lambda col: col.map(lambda x: str(x).strip() if pd.notna(x) else x))
    return df

# æŠ“å–å‡½æ•°
def scrape_urls(url_list, group_cols=None, progress_bar=None):
    sheet_data = {}
    all_data = []
    total = len(url_list)

    for i, url in enumerate(url_list):
        if progress_bar:
            progress_bar.progress((i + 1) / total)
        try:
            headers = {"User-Agent": "Mozilla/5.0"}
            r = requests.get(url, headers=headers, timeout=10)
            r.encoding = r.apparent_encoding
            soup = BeautifulSoup(r.text, "html.parser")

            title_tag = soup.find("title")
            title = title_tag.string.strip() if title_tag else "æœªå‘½åç½‘é¡µ"
            safe_title = "".join([c if c not in r'\/:*?"<>|' else "_" for c in title])

            table = soup.find("table")
            if table:
                dfs = pd.read_html(StringIO(str(table)), header=0)
                df = dfs[0]
                df = clean_df(df, group_cols=group_cols)
                sheet_data[safe_title] = df
                all_data.append(df)
        except Exception as e:
            st.warning(f"æŠ“å–å¤±è´¥ {url}: {e}")

    return sheet_data, all_data

# Streamlit ä¸»ç¨‹åº
st.title("ğŸ“Š é«˜æ ¡æ‹›ç”Ÿå½•å–æ•°æ®çˆ¬å–å·¥å…·")

urls_text = st.text_area("è¯·è¾“å…¥å¤šä¸ªç½‘å€ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰")
group_cols_text = st.text_input("è¯·è¾“å…¥éœ€è¦å‰å‘å¡«å……çš„åˆ—åï¼ˆé€—å·åˆ†éš”ï¼Œå¯é€‰ï¼‰")

if st.button("å¼€å§‹æŠ“å–"):
    urls = [u.strip() for u in urls_text.split("\n") if u.strip()]
    group_cols = [c.strip() for c in group_cols_text.split(",") if c.strip()]

    if not urls:
        st.error("è¯·è‡³å°‘è¾“å…¥ä¸€ä¸ªç½‘å€")
    else:
        progress = st.progress(0)
        sheet_data, all_data = scrape_urls(urls, group_cols, progress_bar=progress)

        if sheet_data:
            output = BytesIO()
            with pd.ExcelWriter(output, engine='openpyxl') as writer:
                for sheet_name, df in sheet_data.items():
                    df.to_excel(writer, sheet_name=sheet_name[:31], index=False)
                if all_data:
                    combined_df = pd.concat(all_data, ignore_index=True)
                    combined_df = clean_df(combined_df, group_cols=group_cols)
                    combined_df.to_excel(writer, sheet_name='æ±‡æ€»', index=False)
            output.seek(0)

            st.success(f"æˆåŠŸæŠ“å– {len(sheet_data)} ä¸ªè¡¨æ ¼ï¼")
            st.download_button(
                label="ğŸ“¥ ä¸‹è½½ Excel æ–‡ä»¶",
                data=output,
                file_name="é«˜æ ¡æ‹›ç”Ÿå½•å–æ•°æ®.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )
        else:
            st.warning("æœªæ‰¾åˆ°ä»»ä½•è¡¨æ ¼æ•°æ®ã€‚")
