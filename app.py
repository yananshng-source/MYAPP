import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
import re
import io
import os
import time
from urllib.parse import urljoin

# ------------------------------
# é¡µé¢é…ç½®
# ------------------------------
st.set_page_config(page_title="ç»¼åˆå·¥å…·ç®±", layout="wide")
st.title("ğŸ§° å¤šåŠŸèƒ½æ•°æ®å·¥å…·ç®±")

tab1, tab2, tab3 = st.tabs(["ğŸ“Š æ•°æ®æŠ“å–", "ğŸ–¼ å›¾ç‰‡ä¸‹è½½", "ğŸ“˜ é€‰ç§‘è½¬æ¢"])


# =========================================================
# ğŸ“Š æ¨¡å— 1ï¼šç½‘é¡µè¡¨æ ¼æŠ“å–å™¨
# =========================================================
with tab1:
    st.header("ğŸ“Š é«˜æ ¡ç½‘é¡µè¡¨æ ¼æŠ“å–å™¨")

    urls_text = st.text_area("è¯·è¾“å…¥å¤šä¸ªç½‘é¡µ URLï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰")
    group_cols = st.text_input("åˆ†ç»„å¡«å……åˆ—åï¼ˆå¯é€‰ï¼Œå¤šåˆ—ç”¨é€—å·åˆ†éš”ï¼‰")

    def clean_df(df, group_cols=None):
        if group_cols:
            fill_cols = [c for c in group_cols if c in df.columns]
            if fill_cols:
                df[fill_cols] = df[fill_cols].ffill()
        df = df.apply(lambda col: col.map(lambda x: str(x).strip() if pd.notna(x) else x))
        return df

    def scrape_urls(url_list, group_cols=None):
        sheet_data = {}
        all_data = []
        for url in url_list:
            try:
                r = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=10)
                r.encoding = r.apparent_encoding
                soup = BeautifulSoup(r.text, "html.parser")
                title_tag = soup.find("title")
                title = title_tag.string.strip() if title_tag else "æœªå‘½åç½‘é¡µ"
                safe_title = "".join([c if c not in r'\/:*?"<>|' else "_" for c in title])
                table = soup.find("table")
                if table:
                    dfs = pd.read_html(io.StringIO(str(table)), header=0)
                    df = clean_df(dfs[0], group_cols)
                    sheet_data[safe_title[:31]] = df
                    all_data.append(df)
            except Exception as e:
                st.warning(f"âš ï¸ æŠ“å–å¤±è´¥: {url} ({e})")
        return sheet_data, all_data

    if st.button("å¼€å§‹æŠ“å–"):
        url_list = [u.strip() for u in urls_text.splitlines() if u.strip()]
        group_list = [g.strip() for g in group_cols.split(",") if g.strip()]
        if not url_list:
            st.error("è¯·è¾“å…¥è‡³å°‘ä¸€ä¸ª URLï¼")
        else:
            with st.spinner("æ­£åœ¨æŠ“å–ç½‘é¡µæ•°æ®..."):
                sheet_data, all_data = scrape_urls(url_list, group_cols=group_list)
                if sheet_data:
                    output = io.BytesIO()
                    with pd.ExcelWriter(output, engine='openpyxl') as writer:
                        for name, df in sheet_data.items():
                            df.to_excel(writer, sheet_name=name[:31], index=False)
                        if all_data:
                            pd.concat(all_data).to_excel(writer, sheet_name="æ±‡æ€»", index=False)
                    output.seek(0)
                    st.success(f"æˆåŠŸæŠ“å– {len(sheet_data)} ä¸ªç½‘é¡µè¡¨æ ¼")
                    st.download_button("ğŸ“¥ ä¸‹è½½ç»“æœ", data=output.getvalue(),
                                       file_name="ç½‘é¡µæŠ“å–ç»“æœ.xlsx",
                                       mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
                else:
                    st.warning("æœªæ‰¾åˆ°ä»»ä½•è¡¨æ ¼æ•°æ®ã€‚")


# =========================================================
# ğŸ–¼ æ¨¡å— 2ï¼šç½‘é¡µå›¾ç‰‡ä¸‹è½½å™¨
# =========================================================
with tab2:
    st.header("ğŸ–¼ å¤šç½‘é¡µå›¾ç‰‡æ‰¹é‡ä¸‹è½½å™¨")

    urls_text = st.text_area("è¯·è¾“å…¥ç½‘é¡µ URLï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰", key="img_urls")
    delay = st.number_input("æ¯é¡µä¸‹è½½é—´éš”ï¼ˆç§’ï¼‰", min_value=0.0, max_value=10.0, value=1.0)

    if st.button("å¼€å§‹ä¸‹è½½å›¾ç‰‡"):
        urls = [u.strip() for u in urls_text.splitlines() if u.strip()]
        if not urls:
            st.error("è¯·è¾“å…¥è‡³å°‘ä¸€ä¸ª URLï¼")
        else:
            os.makedirs("downloaded_images", exist_ok=True)
            downloaded = []
            for idx, url in enumerate(urls, start=1):
                st.write(f"ğŸ”— æ­£åœ¨å¤„ç†ç¬¬ {idx} ä¸ªç½‘é¡µ: {url}")
                try:
                    resp = requests.get(url, timeout=10)
                    soup = BeautifulSoup(resp.content, "html.parser")
                    imgs = soup.find_all("img")
                    for i, img in enumerate(imgs, start=1):
                        src = img.get("src") or img.get("data-src") or img.get("data-original")
                        if not src:
                            continue
                        full_url = urljoin(url, src)
                        r_img = requests.get(full_url, timeout=10)
                        filename = f"page{idx}_img{i}.jpg"
                        with open(os.path.join("downloaded_images", filename), "wb") as f:
                            f.write(r_img.content)
                        downloaded.append(filename)
                    st.success(f"âœ… ç½‘é¡µ {idx} ä¸‹è½½ {len(imgs)} å¼ å›¾ç‰‡")
                except Exception as e:
                    st.warning(f"âš ï¸ ä¸‹è½½å¤±è´¥: {e}")
                time.sleep(delay)
            st.info(f"å…±ä¸‹è½½ {len(downloaded)} å¼ å›¾ç‰‡ï¼Œå·²ä¿å­˜åˆ°é¡¹ç›®ç›®å½•ä¸‹çš„ downloaded_images æ–‡ä»¶å¤¹ã€‚")


# =========================================================
# ğŸ“˜ æ¨¡å— 3ï¼šé€‰ç§‘è¦æ±‚ Excel è½¬æ¢å™¨
# =========================================================
with tab3:
    st.header("ğŸ“˜ é«˜æ ¡é€‰ç§‘è¦æ±‚è½¬æ¢å·¥å…·")

    uploaded_file = st.file_uploader("ä¸Šä¼  Excel æ–‡ä»¶", type=["xlsx", "xls"])

    def convert_selection_requirements(df, original_col='é€‰ç§‘è¦æ±‚', category_col='æ‹›ç”Ÿç§‘ç±»'):
        subject_mapping = {
            'ç‰©ç†': 'ç‰©', 'åŒ–å­¦': 'åŒ–', 'ç”Ÿç‰©': 'ç”Ÿ',
            'å†å²': 'å†', 'åœ°ç†': 'åœ°', 'æ”¿æ²»': 'æ”¿', 'æ€æƒ³æ”¿æ²»': 'æ”¿'
        }

        def extract_subjects(text):
            if pd.isna(text) or text == '' or text == 'ä¸é™':
                return []
            text_str = str(text)
            for full, short in subject_mapping.items():
                text_str = text_str.replace(full, short)
            pattern = r'[ç‰©åŒ–ç”Ÿå†åœ°æ”¿]'
            return list(dict.fromkeys(re.findall(pattern, text_str)))

        def determine_first_selection(subjects, original_text):
            if pd.isna(original_text) or original_text == '':
                return ''
            original_text = str(original_text)
            for sub, short in subject_mapping.items():
                if f'é¦–é€‰{sub}' in original_text:
                    return short
            if 'å†å²ç±»' in original_text:
                return 'å†'
            elif 'ç‰©ç†ç±»' in original_text:
                return 'ç‰©'
            return ''

        def determine_selection_requirement(subjects, first_selection, original_text):
            if pd.isna(original_text) or original_text == '':
                return ''
            original_text = str(original_text)
            if 'ä¸é™' in original_text:
                return 'ä¸é™ç§‘ç›®ä¸“ä¸šç»„'
            remaining = [s for s in subjects if s != first_selection]
            if any(k in original_text for k in ['å’Œ', 'ä¸”', 'å¿…é€‰', 'ã€', '+']) or len(remaining) >= 2:
                return 'å•ç§‘ã€å¤šç§‘å‡éœ€é€‰è€ƒ'
            elif any(k in original_text for k in ['æˆ–', '/', 'é€‰è€ƒä¸€é—¨', 'ä»»é€‰']):
                return 'å¤šé—¨é€‰è€ƒ'
            return 'å•ç§‘ã€å¤šç§‘å‡éœ€é€‰è€ƒ'

        def extract_second_selection(subjects, first_selection):
            remaining = [s for s in subjects if s != first_selection]
            order = ['ç‰©', 'åŒ–', 'ç”Ÿ', 'å†', 'åœ°', 'æ”¿']
            return ''.join([s for s in order if s in remaining])

        result_df = df.copy()
        result_df['é¦–é€‰ç§‘ç›®'] = ''
        result_df['é€‰ç§‘è¦æ±‚ç±»å‹'] = ''
        result_df['æ¬¡é€‰'] = ''

        for idx, row in df.iterrows():
            original_text = row.get(original_col, '')
            category = row.get(category_col, '')
            subjects = extract_subjects(original_text)
            first = determine_first_selection(subjects, original_text)
            if not first and 'ç‰©ç†' in str(category):
                first = 'ç‰©'
            elif not first and 'å†å²' in str(category):
                first = 'å†'
            req = determine_selection_requirement(subjects, first, original_text)
            second = extract_second_selection(subjects, first)
            result_df.at[idx, 'é¦–é€‰ç§‘ç›®'] = first
            result_df.at[idx, 'é€‰ç§‘è¦æ±‚ç±»å‹'] = req
            result_df.at[idx, 'æ¬¡é€‰'] = second
        return result_df

    if uploaded_file:
        df = pd.read_excel(uploaded_file)
        st.dataframe(df.head())
        original_col = st.selectbox("é€‰æ‹©â€˜é€‰ç§‘è¦æ±‚â€™åˆ—", options=df.columns)
        category_col = st.selectbox("é€‰æ‹©â€˜æ‹›ç”Ÿç§‘ç±»â€™åˆ—", options=["(æ— )"] + list(df.columns))
        category_col = "" if category_col == "(æ— )" else category_col
        if st.button("å¼€å§‹è½¬æ¢"):
            result_df = convert_selection_requirements(df, original_col, category_col)
            st.dataframe(result_df.head())
            output = io.BytesIO()
            result_df.to_excel(output, index=False)
            st.download_button("ğŸ“¥ ä¸‹è½½è½¬æ¢ç»“æœ", data=output.getvalue(),
                               file_name="è½¬æ¢ç»“æœ.xlsx",
                               mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
