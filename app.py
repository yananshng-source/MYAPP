# app.py
import streamlit as st
import os
import pandas as pd
from io import BytesIO
from PIL import Image, ImageOps, ImageEnhance
import pytesseract
import requests
from urllib.parse import urljoin
from bs4 import BeautifulSoup
import re
from datetime import datetime
import logging
import traceback
from typing import Iterable, Any

# ------------------------ Config ------------------------
st.set_page_config(page_title="ç»¼åˆå¤„ç†å·¥å…·ç®±", layout="wide")
DEFAULT_TIMEOUT = 15
REQUEST_HEADERS = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
VERIFY_SSL = False
MAX_LOG_LINES = 200

# é…ç½® tesseract è·¯å¾„ï¼ˆä¿®æ”¹ä¸ºä½ æœ¬åœ°è·¯å¾„ï¼‰
pytesseract.pytesseract.tesseract_cmd = r"E:\tesseract-ocr\tesseract.exe"

# ------------------------ Logging ------------------------
logger = logging.getLogger("ç»¼åˆå¤„ç†å·¥å…·ç®±")
logger.setLevel(logging.INFO)
if not logger.handlers:
    ch = logging.StreamHandler()
    ch.setFormatter(logging.Formatter("%(asctime)s - %(levelname)s - %(message)s"))
    logger.addHandler(ch)

if "recent_logs" not in st.session_state:
    st.session_state.recent_logs = []

def log(msg, level="info"):
    entry = f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - {level.upper()} - {msg}"
    st.session_state.recent_logs.append(entry)
    if len(st.session_state.recent_logs) > MAX_LOG_LINES:
        st.session_state.recent_logs = st.session_state.recent_logs[-MAX_LOG_LINES:]
    if level == "info":
        logger.info(msg)
    elif level == "warning":
        logger.warning(msg)
    elif level == "error":
        logger.error(msg)
    else:
        logger.debug(msg)

def progress_iter(it: Iterable[Any], text="å¤„ç†ä¸­...", progress_key=None):
    items = list(it)
    total = len(items)
    if progress_key is None:
        progress_key = "main_progress"
    progress_bar = st.session_state.get(progress_key)
    if progress_bar is None:
        progress_bar = st.progress(0, text=text)
        st.session_state[progress_key] = progress_bar
    try:
        for idx, item in enumerate(items):
            yield item
            percent = int((idx + 1) / total * 100) if total > 0 else 100
            try:
                progress_bar.progress(percent, text=text)
            except Exception:
                pass
        try:
            progress_bar.progress(100, text=text + " âœ… å®Œæˆ")
        except Exception:
            pass
    finally:
        if progress_key in st.session_state:
            del st.session_state[progress_key]

def safe_requests_get(session: requests.Session, url: str, **kwargs):
    try:
        resp = session.get(url, timeout=kwargs.get("timeout", DEFAULT_TIMEOUT),
                           headers=REQUEST_HEADERS, verify=VERIFY_SSL)
        resp.raise_for_status()
        return resp
    except Exception as e:
        raise

def ensure_dir(path):
    os.makedirs(path, exist_ok=True)
    return path

# ------------------------ æ ¸å¿ƒåŠŸèƒ½ ------------------------
# Tab1: ç½‘é¡µè¡¨æ ¼æŠ“å–
def scrape_table(url_list, group_cols):
    session = requests.Session()
    sheet_data = {}
    all_data = []
    errors = []

    enumerated = list(enumerate(url_list, start=1))
    for idx, url in progress_iter(enumerated, text="æŠ“å–ç½‘é¡µè¡¨æ ¼ä¸­"):
        try:
            resp = safe_requests_get(session, url)
            dfs = pd.read_html(resp.text)
            for i, df in enumerate(dfs):
                name = f"ç½‘é¡µ{idx}_è¡¨{i+1}"
                sheet_data[name] = df
                all_data.append(df)
                log(f"æŠ“å–åˆ°è¡¨æ ¼: {name} ({len(df)} è¡Œ)")
        except Exception as e:
            log(f"æŠ“å– URL å¤±è´¥: {url} -> {e}", level="warning")
            continue

    if sheet_data:
        output = BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            for name, df in sheet_data.items():
                df.to_excel(writer, sheet_name=name[:31], index=False)
            if all_data:
                try:
                    pd.concat(all_data, ignore_index=True).to_excel(writer, sheet_name="æ±‡æ€»", index=False)
                except:
                    pass
        output.seek(0)
        return output
    else:
        log("æœªæŠ“å–åˆ°ä»»ä½•è¡¨æ ¼ã€‚", level="warning")
        return None

# Tab2: ç½‘é¡µå›¾ç‰‡ä¸‹è½½
def download_images_from_urls(url_list, output_dir=None):
    if output_dir is None:
        output_dir = os.path.join(os.path.expanduser("~"), "Desktop", "downloaded_images")
    ensure_dir(output_dir)
    session = requests.Session()
    session.headers.update(REQUEST_HEADERS)
    downloaded_files = []

    enumerated = list(enumerate(url_list, start=1))
    for idx, url in progress_iter(enumerated, text="ä¸‹è½½ç½‘é¡µå›¾ç‰‡ä¸­"):
        try:
            resp = safe_requests_get(session, url)
            soup = BeautifulSoup(resp.content, "html.parser")
            imgs = soup.find_all("img")
            for i, img_tag in enumerate(imgs, start=1):
                src = img_tag.get("src") or img_tag.get("data-src") or img_tag.get("data-original")
                if not src:
                    continue
                full_url = urljoin(url, src.strip())
                try:
                    resp_img = safe_requests_get(session, full_url)
                    ext = os.path.splitext(full_url)[1]
                    if not ext or len(ext) > 6:
                        ext = ".jpg"
                    fname = f"img_{idx}_{i}{ext}"
                    fpath = os.path.join(output_dir, fname)
                    with open(fpath, "wb") as f:
                        f.write(resp_img.content)
                    downloaded_files.append(fpath)
                except:
                    continue
        except:
            continue
    return output_dir, downloaded_files

# Tab3: å›¾ç‰‡è£å‰ª+OCRé¡µç é‡å‘½å
def crop_and_ocr_images(folder_path, x_center, y_center, crop_width, crop_height):
    output_folder = os.path.join(os.path.expanduser("~"), "Desktop", "crop_results")
    ensure_dir(output_folder)
    img_exts = (".png", ".jpg", ".jpeg", ".bmp", ".tif", ".tiff")
    files = [f for f in os.listdir(folder_path) if f.lower().endswith(img_exts)]
    used_pages = set()
    failed_files = []
    for filename in progress_iter(files, text="è£å‰ª+OCRè¯†åˆ«ä¸­"):
        try:
            img_path = os.path.join(folder_path, filename)
            img = Image.open(img_path).convert("RGB")
            width, height = img.size
            left = max(0, int(x_center - crop_width // 2))
            right = min(width, int(x_center + crop_width // 2))
            top = max(0, int(y_center - crop_height // 2))
            bottom = min(height, int(y_center + crop_height // 2))
            crop_img = img.crop((left, top, right, bottom))
            crop_img = crop_img.resize((crop_img.width*2, crop_img.height*2), Image.LANCZOS)
            gray = ImageOps.grayscale(crop_img)
            gray = ImageEnhance.Contrast(gray).enhance(3.0)
            bw = gray.point(lambda x: 0 if x < 128 else 255, '1')
            text = pytesseract.image_to_string(bw, config='--psm 7 -c tessedit_char_whitelist=0123456789')
            matches = re.findall(r'\d+', text)
            if matches:
                page_number = int(matches[-1])
                while page_number in used_pages:
                    page_number += 1
                used_pages.add(page_number)
            else:
                failed_files.append(filename)
                page_number = max(used_pages) + 1 if used_pages else 1
                used_pages.add(page_number)
            ext = os.path.splitext(filename)[1]
            new_name = f"{page_number:03d}{ext}"
            new_path = os.path.join(folder_path, new_name)
            os.rename(img_path, new_path)
            crop_save_path = os.path.join(output_folder, f"crop_{new_name}")
            bw.save(crop_save_path)
        except:
            failed_files.append(filename)
            continue
    return output_folder, failed_files

# Tab4: é«˜æ ¡é€‰ç§‘è½¬æ¢
def convert_selection_requirements(df):
    subject_mapping = {'ç‰©ç†': 'ç‰©', 'åŒ–å­¦': 'åŒ–', 'ç”Ÿç‰©': 'ç”Ÿ', 'å†å²': 'å†', 'åœ°ç†': 'åœ°', 'æ”¿æ²»': 'æ”¿',
                       'æ€æƒ³æ”¿æ²»': 'æ”¿'}
    df_new = df.copy()
    df_new['é¦–é€‰ç§‘ç›®'] = ''
    df_new['é€‰ç§‘è¦æ±‚ç±»å‹'] = ''
    df_new['æ¬¡é€‰'] = ''
    for idx, row in progress_iter(list(df.iterrows()), text="é€‰ç§‘è½¬æ¢ä¸­"):
        try:
            i, r = row
            text = str(r.get('é€‰ç§‘è¦æ±‚', '')).strip()
            cat = str(r.get('æ‹›ç”Ÿç§‘ç±»', '')).strip()
            subjects = [subject_mapping.get(s, s) for s in re.findall(r'ç‰©ç†|åŒ–å­¦|ç”Ÿç‰©|å†å²|åœ°ç†|æ”¿æ²»|æ€æƒ³æ”¿æ²»', text)]
            first = ''
            for s_full, s_short in subject_mapping.items():
                if f'é¦–é€‰{s_full}' in text:
                    first = s_short
            if not first:
                if 'ç‰©ç†' in cat:
                    first = 'ç‰©'
                elif 'å†å²' in cat:
                    first = 'å†'
            remaining = [s for s in subjects if s != first]
            second = ''.join(remaining)
            if 'ä¸é™' in text:
                req_type = 'ä¸é™ç§‘ç›®ä¸“ä¸šç»„'
            elif len(remaining) >= 1:
                req_type = 'å¤šé—¨é€‰è€ƒ'
            else:
                req_type = 'å•ç§‘ã€å¤šç§‘å‡éœ€é€‰è€ƒ'
            df_new.at[i, 'é¦–é€‰ç§‘ç›®'] = first
            df_new.at[i, 'æ¬¡é€‰'] = second
            df_new.at[i, 'é€‰ç§‘è¦æ±‚ç±»å‹'] = req_type
        except:
            continue
    return df_new

# Tab5: Excelæ—¥æœŸå¤„ç†
def safe_parse_datetime(datetime_str, year):
    if pd.isna(datetime_str):
        return None
    datetime_str = str(datetime_str).strip()
    if not re.search(r'(^|\D)\d{4}(\D|$)', datetime_str):
        datetime_str = f"{year}å¹´{datetime_str}"
    patterns = [(r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥(\d{1,2}):(\d{1,2})', '%Yå¹´%mæœˆ%dæ—¥%H:%M'),
                (r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥', '%Yå¹´%mæœˆ%dæ—¥'),
                (r'(\d{4})-(\d{1,2})-(\d{1,2})', '%Y-%m-%d'),
                (r'(\d{4})/(\d{1,2})/(\d{1,2})', '%Y/%m/%d')]
    for pattern, fmt in patterns:
        try:
            dt = datetime.strptime(datetime_str, fmt)
            return dt
        except:
            continue
    return None

def process_date_range(date_str, year):
    if pd.isna(date_str):
        return date_str, "", ""
    date_str = str(date_str).strip()
    if '-' in date_str:
        start_str, end_str = date_str.split('-', 1)
        start_dt = safe_parse_datetime(start_str, year)
        end_dt = safe_parse_datetime(end_str, year)
        if not start_dt or not end_dt:
            return date_str, "æ ¼å¼é”™è¯¯", "æ ¼å¼é”™è¯¯"
        if ':' not in start_str:
            start_dt = start_dt.replace(hour=0, minute=0, second=0)
        if ':' not in end_str:
            end_dt = end_dt.replace(hour=23, minute=59, second=59)
        return date_str, start_dt, end_dt
    else:
        dt = safe_parse_datetime(date_str, year)
        return date_str, dt, dt

# ------------------------ Streamlit UI ------------------------
st.title("ğŸ§° ç»¼åˆå¤„ç†å·¥å…·ç®± - å®Œæ•´ç‰ˆï¼ˆå¸¦è¿›åº¦æ¡ & æ—¥å¿—ï¼‰")
tab1, tab2, tab3, tab4, tab5 = st.tabs([
    "ç½‘é¡µè¡¨æ ¼æŠ“å–",
    "ç½‘é¡µå›¾ç‰‡ä¸‹è½½",
    "å›¾ç‰‡è£å‰ª+OCRé¡µç ",
    "é«˜æ ¡é€‰ç§‘è½¬æ¢",
    "Excelæ—¥æœŸå¤„ç†"
])

with st.sidebar.expander("è¿è¡Œæ—¥å¿—ï¼ˆæœ€æ–°ï¼‰", expanded=True):
    for line in st.session_state.recent_logs[-200:]:
        st.text(line)

# ------------------------ Tab1: ç½‘é¡µè¡¨æ ¼æŠ“å– ------------------------
with tab1:
    st.subheader("ç½‘é¡µè¡¨æ ¼æŠ“å–")
    urls_text = st.text_area("è¾“å…¥ç½‘é¡µURLï¼Œæ¯è¡Œä¸€ä¸ª", height=150)
    if st.button("æŠ“å–è¡¨æ ¼", key="scrape_table_btn"):
        urls = [u.strip() for u in urls_text.strip().splitlines() if u.strip()]
        if urls:
            excel_bytes = scrape_table(urls, group_cols=[])
            if excel_bytes:
                st.download_button("ä¸‹è½½æŠ“å–ç»“æœ", data=excel_bytes, file_name="æŠ“å–ç»“æœ.xlsx")
            else:
                st.warning("æœªæŠ“å–åˆ°è¡¨æ ¼")
        else:
            st.warning("è¯·æä¾›æœ‰æ•ˆURLåˆ—è¡¨")

# ------------------------ Tab2: ç½‘é¡µå›¾ç‰‡ä¸‹è½½ ------------------------
with tab2:
    st.subheader("ç½‘é¡µå›¾ç‰‡ä¸‹è½½")
    urls_text2 = st.text_area("è¾“å…¥ç½‘é¡µURLï¼Œæ¯è¡Œä¸€ä¸ª", height=150, key="img_urls")
    if st.button("ä¸‹è½½å›¾ç‰‡", key="download_imgs_btn"):
        urls = [u.strip() for u in urls_text2.strip().splitlines() if u.strip()]
        if urls:
            folder, files = download_images_from_urls(urls)
            st.success(f"ä¸‹è½½å®Œæˆï¼Œä¿å­˜åˆ°: {folder}")
            st.write(f"ä¸‹è½½å›¾ç‰‡æ•°é‡: {len(files)}")
        else:
            st.warning("è¯·æä¾›æœ‰æ•ˆURLåˆ—è¡¨")

# ------------------------ Tab3: å›¾ç‰‡è£å‰ª+OCRé¡µç  ------------------------
with tab3:
    st.subheader("å›¾ç‰‡è£å‰ª + OCRé¡µç è¯†åˆ«é‡å‘½å")
    folder_path = st.text_input("å›¾ç‰‡æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆç»å¯¹è·¯å¾„ï¼‰", key="img_folder_ocr")
    x_center = st.number_input("é¡µç ä¸­å¿ƒX", value=788, key="x_center_ocr")
    y_center = st.number_input("é¡µç ä¸­å¿ƒY", value=1955, key="y_center_ocr")
    crop_w = st.number_input("è£å‰ªå®½åº¦(px)", value=200, key="crop_w_ocr")
    crop_h = st.number_input("è£å‰ªé«˜åº¦(px)", value=50, key="crop_h_ocr")
    if st.button("è£å‰ªå¹¶è¯†åˆ«é¡µç ", key="crop_ocr_btn"):
        folder_path = folder_path.strip()
        if not folder_path or not os.path.isdir(folder_path):
            st.warning(f"è¯·æä¾›æœ‰æ•ˆå›¾ç‰‡æ–‡ä»¶å¤¹è·¯å¾„ï¼š{folder_path}")
        else:
            output_folder, failed_files = crop_and_ocr_images(folder_path, x_center, y_center, crop_w, crop_h)
            st.success(f"å®Œæˆï¼è£å‰ªç»“æœå·²ä¿å­˜åˆ°æ¡Œé¢: {output_folder}ï¼ŒåŸå›¾ç‰‡å·²æŒ‰é¡µç é‡å‘½å")
            if failed_files:
                st.warning(f"OCRè¯†åˆ«å¤±è´¥çš„å›¾ç‰‡: {', '.join(failed_files)}")

# ------------------------ Tab4: é«˜æ ¡é€‰ç§‘è½¬æ¢ ------------------------
with tab4:
    st.subheader("é«˜æ ¡é€‰ç§‘è½¬æ¢")
    uploaded_file = st.file_uploader("ä¸Šä¼  Excel æ–‡ä»¶", type=["xlsx"])
    if uploaded_file:
        df = pd.read_excel(uploaded_file)
        df_new = convert_selection_requirements(df)
        st.dataframe(df_new.head(10))
        output = BytesIO()
        df_new.to_excel(output, index=False)
        output.seek(0)
        st.download_button("ä¸‹è½½è½¬æ¢ç»“æœ", data=output, file_name="é€‰ç§‘è½¬æ¢ç»“æœ.xlsx")

# ------------------------ Tab5: Excelæ—¥æœŸå¤„ç† ------------------------
with tab5:
    st.subheader("Excelæ—¥æœŸå¤„ç†")
    uploaded_file2 = st.file_uploader("ä¸Šä¼  Excel æ–‡ä»¶", type=["xlsx"], key="date_file")
    year_input = st.number_input("é»˜è®¤å¹´ä»½", value=datetime.now().year)
    if uploaded_file2:
        df_date = pd.read_excel(uploaded_file2)
        for col in df_date.columns:
            df_date[[f"{col}_åŸ", f"{col}_å¼€å§‹", f"{col}_ç»“æŸ"]] = df_date[col].apply(lambda x: pd.Series(process_date_range(x, year_input)))
        st.dataframe(df_date.head(10))
        output2 = BytesIO()
        df_date.to_excel(output2, index=False)
        output2.seek(0)
        st.download_button("ä¸‹è½½å¤„ç†ç»“æœ", data=output2, file_name="æ—¥æœŸå¤„ç†ç»“æœ.xlsx")
