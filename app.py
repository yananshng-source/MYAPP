# app.py
import streamlit as st
import os
import pandas as pd
from io import BytesIO
from PIL import Image, ImageOps, ImageEnhance
import pytesseract
import re
import requests
from urllib.parse import urljoin
from bs4 import BeautifulSoup

st.set_page_config(page_title="ç»¼åˆå¤„ç†å·¥å…·ç®±", layout="wide")
st.title("ğŸ§° ç»¼åˆå¤„ç†å·¥å…·ç®± - ç»Ÿä¸€ç•Œé¢ç‰ˆ")

# ------------------------ è¾“å…¥åŒºåŸŸ ------------------------
st.header("è¾“å…¥å‚æ•°è®¾ç½®")

# 1ï¸âƒ£ URLåˆ—è¡¨
urls_text = st.text_area("ç½‘é¡µURLåˆ—è¡¨ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰", height=120)

# 2ï¸âƒ£ å›¾ç‰‡OCRå‚æ•°
st.subheader("å›¾ç‰‡OCRå‚æ•°")
img_folder = st.text_input("å›¾ç‰‡æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆç»å¯¹è·¯å¾„ï¼‰")
tess_path = st.text_input("Tesseractè·¯å¾„", value=r"E:\tesseract-ocr\tesseract.exe")
x_center = st.number_input("é¡µç ä¸­å¿ƒX", value=788)
y_center = st.number_input("é¡µç ä¸­å¿ƒY", value=1955)
crop_w = st.number_input("è£å‰ªå®½åº¦(px)", value=200)
crop_h = st.number_input("è£å‰ªé«˜åº¦(px)", value=50)

# 3ï¸âƒ£ Excelæ–‡ä»¶
uploaded_file = st.file_uploader("ä¸Šä¼ Excelæ–‡ä»¶ï¼ˆé€‰ç§‘è½¬æ¢/æ—¥æœŸå¤„ç†é€šç”¨ï¼‰", type=["xlsx","xls"])

# 4ï¸âƒ£ é€‰ç§‘è½¬æ¢å‚æ•°
group_cols = st.text_input("è¡¨æ ¼æŠ“å–åˆ†ç»„åˆ—ï¼ˆé€—å·åˆ†éš”ï¼Œå¯é€‰ï¼‰")
date_col = st.text_input("æ—¥æœŸåˆ—åï¼ˆç”¨äºæ—¥æœŸå¤„ç†ï¼‰", value="æ—¥æœŸ")
year = st.number_input("æŒ‡å®šå¹´ä»½ï¼ˆæ—¥æœŸå¤„ç†ç”¨ï¼‰", value=2025)

# 5ï¸âƒ£ åŠŸèƒ½é€‰æ‹©
st.subheader("é€‰æ‹©è¦æ‰§è¡Œçš„åŠŸèƒ½")
modules = st.multiselect("åŠŸèƒ½æ¨¡å—", [
    "ç½‘é¡µè¡¨æ ¼æŠ“å–",
    "ç½‘é¡µå›¾ç‰‡ä¸‹è½½",
    "å›¾ç‰‡OCRè£å‰ª",
    "é«˜æ ¡é€‰ç§‘è½¬æ¢",
    "Excelæ—¥æœŸå¤„ç†"
])

# ------------------------ åŠŸèƒ½å‡½æ•° ------------------------
def scrape_table(url_list, group_cols):
    from modules.table_scraper import scrape_urls
    group_list = [g.strip() for g in group_cols.split(",") if g.strip()]
    sheet_data, all_data = scrape_urls(url_list, group_cols=group_list)
    if sheet_data:
        output = BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            for name, df in sheet_data.items():
                df.to_excel(writer, sheet_name=name[:31], index=False)
            if all_data:
                pd.concat(all_data).to_excel(writer, sheet_name="æ±‡æ€»", index=False)
        output.seek(0)
        return output
    return None

def download_images_from_urls(url_list, output_dir=None):
    if output_dir is None:
        output_dir = os.path.join(os.path.expanduser("~"), "Desktop", "downloaded_images")
    os.makedirs(output_dir, exist_ok=True)
    session = requests.Session()
    session.headers.update({"User-Agent": "Mozilla/5.0"})
    img_exts = (".png", ".jpg", ".jpeg", ".bmp", ".tif", ".tiff")
    downloaded_files = []
    for idx, url in enumerate(url_list, start=1):
        try:
            r = session.get(url, timeout=10)
            r.raise_for_status()
            soup = BeautifulSoup(r.content, "html.parser")
            title_tag = soup.find("title")
            title = title_tag.string.strip() if title_tag else f"ç½‘é¡µ{idx}"
            safe_title = "".join([c if c not in r'\/:*?"<>|' else "_" for c in title])
            imgs = soup.find_all("img")
            for i, img_tag in enumerate(imgs, start=1):
                src = img_tag.get("src") or img_tag.get("data-src") or img_tag.get("data-original")
                if not src:
                    continue
                full_url = urljoin(url, src.strip())
                try:
                    resp_img = session.get(full_url, timeout=10)
                    resp_img.raise_for_status()
                    ext = os.path.splitext(full_url)[1]
                    if not ext.lower() in img_exts:
                        ext = ".jpg"
                    fname = f"{safe_title}_{i}{ext}"
                    fpath = os.path.join(output_dir, fname)
                    with open(fpath, "wb") as f:
                        f.write(resp_img.content)
                    downloaded_files.append(fpath)
                except:
                    continue
        except:
            continue
    return output_dir, downloaded_files

def crop_and_rename_images(folder_path, x_center, y_center, crop_width, crop_height, tesseract_path):
    pytesseract.pytesseract.tesseract_cmd = tesseract_path
    output_folder = os.path.join(os.path.expanduser("~"), "Desktop", "crop_results")
    os.makedirs(output_folder, exist_ok=True)
    used_pages = set()
    img_exts = (".png", ".jpg", ".jpeg", ".bmp", ".tif", ".tiff")
    for filename in os.listdir(folder_path):
        if filename.lower().endswith(img_exts):
            try:
                image_path = os.path.join(folder_path, filename)
                img = Image.open(image_path).convert("RGB")
                width, height = img.size
                left = max(0, x_center - crop_width // 2)
                right = min(width, x_center + crop_width // 2)
                top = max(0, y_center - crop_height // 2)
                bottom = min(height, y_center + crop_height // 2)
                crop_img = img.crop((left, top, right, bottom))
                crop_img = crop_img.resize((crop_img.width*2, crop_img.height*2), Image.LANCZOS)
                gray = ImageOps.grayscale(crop_img)
                gray = ImageEnhance.Contrast(gray).enhance(3.0)
                bw = gray.point(lambda x: 0 if x < 128 else 255, '1')
                text = pytesseract.image_to_string(
                    bw, config='--psm 7 -c tessedit_char_whitelist=0123456789'
                )
                matches = re.findall(r'\d+', text)
                if matches:
                    page_number = int(matches[-1])
                    while page_number in used_pages:
                        page_number += 1
                    used_pages.add(page_number)
                else:
                    page_number = max(used_pages)+1 if used_pages else 1
                    used_pages.add(page_number)
                ext = os.path.splitext(filename)[1]
                new_name = f"{page_number:03d}{ext}"
                new_path = os.path.join(folder_path, new_name)
                os.rename(image_path, new_path)
                crop_save_path = os.path.join(output_folder, f"crop_{new_name}")
                bw.save(crop_save_path)
            except Exception as e:
                st.warning(f"{filename} å¤„ç†å¤±è´¥: {e}")
    return output_folder

# ------------------------ æ‰§è¡ŒæŒ‰é’® ------------------------
if st.button("æ‰§è¡Œé€‰ä¸­æ¨¡å—"):
    # URLåˆ—è¡¨
    url_list = [u.strip() for u in urls_text.splitlines() if u.strip()] if urls_text else []

    # Excelä¸´æ—¶ä¿å­˜
    temp_excel_path = None
    if uploaded_file:
        temp_excel_path = os.path.join("temp_uploaded.xlsx")
        with open(temp_excel_path, "wb") as f:
            f.write(uploaded_file.getbuffer())

    # æ‰§è¡Œæ¨¡å—
    if "ç½‘é¡µè¡¨æ ¼æŠ“å–" in modules and url_list:
        st.subheader("ç½‘é¡µè¡¨æ ¼æŠ“å–ç»“æœ")
        output = scrape_table(url_list, group_cols)
        if output:
            st.download_button("ä¸‹è½½æŠ“å–è¡¨æ ¼", data=output.getvalue(), file_name="ç½‘é¡µæŠ“å–.xlsx")
        else:
            st.warning("æœªæŠ“å–åˆ°è¡¨æ ¼æ•°æ®")

    if "ç½‘é¡µå›¾ç‰‡ä¸‹è½½" in modules and url_list:
        st.subheader("ç½‘é¡µå›¾ç‰‡ä¸‹è½½")
        output_dir, files = download_images_from_urls(url_list)
        st.success(f"å®Œæˆï¼å…±ä¸‹è½½ {len(files)} å¼ å›¾ç‰‡ï¼Œä¿å­˜åˆ°: {output_dir}")

    if "å›¾ç‰‡OCRè£å‰ª" in modules and img_folder:
        st.subheader("å›¾ç‰‡OCRè£å‰ª")
        if not os.path.exists(img_folder):
            st.error("å›¾ç‰‡æ–‡ä»¶å¤¹è·¯å¾„æ— æ•ˆ")
        else:
            output_folder = crop_and_rename_images(img_folder, x_center, y_center, crop_w, crop_h, tess_path)
            st.success(f"å®Œæˆï¼è£å‰ªç»“æœå·²ä¿å­˜åˆ°æ¡Œé¢ï¼š{output_folder}")

    if "é«˜æ ¡é€‰ç§‘è½¬æ¢" in modules and temp_excel_path:
        st.subheader("é«˜æ ¡é€‰ç§‘è½¬æ¢")
        from modules.selection_processor import process_excel as selection_excel
        out_path, df = selection_excel(temp_excel_path)
        st.dataframe(df.head(10))
        st.download_button("ä¸‹è½½è½¬æ¢ç»“æœ", open(out_path,"rb"), file_name=os.path.basename(out_path))

    if "Excelæ—¥æœŸå¤„ç†" in modules and temp_excel_path:
        st.subheader("Excelæ—¥æœŸå¤„ç†")
        from modules.date_processor import process_excel as date_excel
        output_file = os.path.join(os.path.expanduser("~"), "Desktop", "æ—¥æœŸå¤„ç†ç»“æœ.xlsx")
        date_excel(temp_excel_path, output_file, date_col_name=date_col, year=year)
        st.success(f"å®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ°æ¡Œé¢: {output_file}")
