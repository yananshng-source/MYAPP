# app.py
import streamlit as st
import os
import pandas as pd
from io import BytesIO
from PIL import Image, ImageOps, ImageEnhance
import pytesseract
import re
import requests
from urllib.parse import urljoin
from bs4 import BeautifulSoup
from datetime import datetime
import logging
from typing import Iterable, Any

# ------------------------ Config ------------------------
st.set_page_config(page_title="ç»¼åˆå¤„ç†å·¥å…·ç®±", layout="wide")
DEFAULT_TIMEOUT = 15
REQUEST_HEADERS = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
VERIFY_SSL = False
MAX_LOG_LINES = 200
# ä¿®æ”¹ä¸ºä½ æœ¬åœ° tesseract å¯æ‰§è¡Œæ–‡ä»¶è·¯å¾„
pytesseract.pytesseract.tesseract_cmd = r"E:\tesseract-ocr\tesseract.exe"

# ------------------------ Logging ------------------------
logger = logging.getLogger("ç»¼åˆå¤„ç†å·¥å…·ç®±")
logger.setLevel(logging.INFO)
if not logger.handlers:
    ch = logging.StreamHandler()
    ch.setFormatter(logging.Formatter("%(asctime)s - %(levelname)s - %(message)s"))
    logger.addHandler(ch)

if "recent_logs" not in st.session_state:
    st.session_state.recent_logs = []

def log(msg, level="info"):
    entry = f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - {level.upper()} - {msg}"
    st.session_state.recent_logs.append(entry)
    if len(st.session_state.recent_logs) > MAX_LOG_LINES:
        st.session_state.recent_logs = st.session_state.recent_logs[-MAX_LOG_LINES:]
    if level == "info":
        logger.info(msg)
    elif level == "warning":
        logger.warning(msg)
    elif level == "error":
        logger.error(msg)

def progress_iter(it: Iterable[Any], text="å¤„ç†ä¸­...", progress_key=None):
    items = list(it)
    total = len(items)
    if progress_key is None:
        progress_key = "main_progress"
    progress_bar = st.session_state.get(progress_key)
    if progress_bar is None:
        progress_bar = st.progress(0, text=text)
        st.session_state[progress_key] = progress_bar
    try:
        for idx, item in enumerate(items):
            yield item
            percent = int((idx + 1) / total * 100) if total > 0 else 100
            try:
                progress_bar.progress(percent, text=text)
            except Exception:
                pass
        try:
            progress_bar.progress(100, text=text + " âœ… å®Œæˆ")
        except Exception:
            pass
    finally:
        if progress_key in st.session_state:
            del st.session_state[progress_key]

def ensure_dir(path):
    os.makedirs(path, exist_ok=True)
    return path

# ------------------------ Helpers ------------------------
def safe_requests_get(session: requests.Session, url: str, **kwargs):
    try:
        resp = session.get(url, timeout=kwargs.get("timeout", DEFAULT_TIMEOUT),
                           headers=REQUEST_HEADERS, verify=VERIFY_SSL)
        resp.raise_for_status()
        return resp
    except Exception as e:
        raise

# ------------------------ ç½‘é¡µè¡¨æ ¼æŠ“å– ------------------------
def scrape_table(url_list, group_cols):
    session = requests.Session()
    sheet_data = {}
    all_data = []

    for idx, url in progress_iter(list(enumerate(url_list, start=1)), text="æŠ“å–ç½‘é¡µè¡¨æ ¼ä¸­"):
        try:
            resp = safe_requests_get(session, url)
            dfs = pd.read_html(resp.text)
            for i, df in enumerate(dfs):
                name = f"ç½‘é¡µ{idx}_è¡¨{i + 1}"
                sheet_data[name] = df
                all_data.append(df)
                log(f"æŠ“å–åˆ°è¡¨æ ¼: {name} ({len(df)} è¡Œ)")
        except Exception as e:
            log(f"æŠ“å– URL å¤±è´¥: {url} -> {e}", level="warning")
            continue

    if sheet_data:
        output = BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            for name, df in sheet_data.items():
                safe_name = name[:31]
                df.to_excel(writer, sheet_name=safe_name, index=False)
            if all_data:
                try:
                    pd.concat(all_data, ignore_index=True).to_excel(writer, sheet_name="æ±‡æ€»", index=False)
                except Exception as e:
                    log(f"åˆå¹¶æ±‡æ€»è¡¨å¤±è´¥: {e}", level="warning")
        output.seek(0)
        return output
    else:
        log("æœªæŠ“å–åˆ°ä»»ä½•è¡¨æ ¼ã€‚", level="warning")
        return None

# ------------------------ ç½‘é¡µå›¾ç‰‡ä¸‹è½½ ------------------------
def download_images_from_urls(url_list, output_dir=None):
    if output_dir is None:
        output_dir = os.path.join(os.path.expanduser("~"), "Desktop", "downloaded_images")
    ensure_dir(output_dir)
    session = requests.Session()
    session.headers.update(REQUEST_HEADERS)
    downloaded_files = []

    for idx, url in progress_iter(list(enumerate(url_list, start=1)), text="ä¸‹è½½ç½‘é¡µå›¾ç‰‡ä¸­"):
        try:
            resp = safe_requests_get(session, url)
            soup = BeautifulSoup(resp.content, "html.parser")
            title_tag = soup.find("title")
            title = title_tag.string.strip() if title_tag else f"ç½‘é¡µ{idx}"
            safe_title = "".join([c if c not in r'\/:*?"<>|' else "_" for c in title])
            imgs = soup.find_all("img")
            for i, img_tag in enumerate(imgs, start=1):
                src = img_tag.get("src") or img_tag.get("data-src")
                if not src:
                    continue
                full_url = urljoin(url, src.strip())
                try:
                    resp_img = safe_requests_get(session, full_url)
                    ext = os.path.splitext(full_url)[1]
                    if not ext or len(ext) > 6:
                        ext = ".jpg"
                    fname = f"{safe_title}_{i}{ext}"
                    fpath = os.path.join(output_dir, fname)
                    with open(fpath, "wb") as f:
                        f.write(resp_img.content)
                    downloaded_files.append(fpath)
                except Exception as e:
                    log(f"å›¾ç‰‡ä¸‹è½½å¤±è´¥: {full_url} -> {e}", level="warning")
                    continue
        except Exception as e:
            log(f"é¡µé¢è¯·æ±‚å¤±è´¥: {url} -> {e}", level="warning")
            continue
    return output_dir, downloaded_files

# ------------------------ å›¾ç‰‡è£å‰ª + OCR ------------------------
def crop_and_ocr_images_from_folder(folder_path, x_center, y_center, crop_width, crop_height):
    output_folder = os.path.join(os.path.expanduser("~"), "Desktop", "crop_results")
    ensure_dir(output_folder)
    img_exts = (".png", ".jpg", ".jpeg", ".bmp", ".tif", ".tiff")
    files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.lower().endswith(img_exts)]
    used_pages = set()
    results = []

    for image_path in progress_iter(files, text="è£å‰ªå¹¶è¯†åˆ«é¡µç "):
        try:
            filename = os.path.basename(image_path)
            img = Image.open(image_path).convert("RGB")
            width, height = img.size
            left = max(0, int(x_center - crop_width // 2))
            right = min(width, int(x_center + crop_width // 2))
            top = max(0, int(y_center - crop_height // 2))
            bottom = min(height, int(y_center + crop_height // 2))
            crop_img = img.crop((left, top, right, bottom))
            crop_img = crop_img.resize((crop_img.width * 2, crop_img.height * 2), Image.LANCZOS)
            gray = ImageOps.grayscale(crop_img)
            gray = ImageEnhance.Contrast(gray).enhance(3.0)
            bw = gray.point(lambda x: 0 if x < 128 else 255, '1')

            text = pytesseract.image_to_string(bw, config='--psm 7 -c tessedit_char_whitelist=0123456789')
            matches = re.findall(r'\d+', text)
            if matches:
                page_number = int(matches[-1])
                while page_number in used_pages:
                    page_number += 1
                used_pages.add(page_number)
            else:
                page_number = max(used_pages) + 1 if used_pages else 1
                used_pages.add(page_number)

            ext = os.path.splitext(filename)[1]
            new_name = f"{page_number:03d}{ext}"
            new_path = os.path.join(output_folder, new_name)
            img.save(new_path)
            crop_save_path = os.path.join(output_folder, f"crop_{new_name}")
            bw.save(crop_save_path)
            results.append((filename, new_name))
            log(f"{filename} -> {new_name} ï¼ˆè£å‰ªç»“æœå·²ä¿å­˜ï¼‰")
        except Exception as e:
            log(f"{filename} å¤„ç†å¤±è´¥: {e}", level="warning")
            continue
    return output_folder, results

# ------------------------ æ–‡ä»¶å¤¹é€‰æ‹©å™¨ ------------------------
def folder_selector(label="é€‰æ‹©æ–‡ä»¶å¤¹"):
    from tkinter import Tk, filedialog
    root = Tk()
    root.withdraw()
    path = filedialog.askdirectory(title=label)
    root.destroy()
    return path

# ------------------------ Streamlit UI ------------------------
st.title("ğŸ§° ç»¼åˆå¤„ç†å·¥å…·ç®± - å®Œæ•´ç‰ˆï¼ˆå¸¦OCRé¡µç è¯†åˆ«ï¼‰")
tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs([
    "ç½‘é¡µè¡¨æ ¼æŠ“å–", "ç½‘é¡µå›¾ç‰‡ä¸‹è½½", "å›¾ç‰‡è£å‰ª+OCR", "é«˜æ ¡é€‰ç§‘è½¬æ¢", "Excelæ—¥æœŸå¤„ç†", "è¿è¡Œæ—¥å¿—"
])

# ------------------------ Tab3 æ–‡ä»¶å¤¹é€‰æ‹© ------------------------
with tab3:
    st.subheader("å›¾ç‰‡è£å‰ª + OCRé¡µç é‡å‘½å")
    folder_path_input = st.text_input("é€‰æ‹©å›¾ç‰‡æ–‡ä»¶å¤¹ï¼ˆç‚¹å‡»æŒ‰é’®é€‰æ‹©ï¼‰")
    if st.button("é€‰æ‹©æ–‡ä»¶å¤¹"):
        folder_path_input = folder_selector()
        st.text_input("é€‰æ‹©å›¾ç‰‡æ–‡ä»¶å¤¹ï¼ˆç‚¹å‡»æŒ‰é’®é€‰æ‹©ï¼‰", value=folder_path_input, key="folder_path_display")
    x_center = st.number_input("é¡µç ä¸­å¿ƒX", value=788)
    y_center = st.number_input("é¡µç ä¸­å¿ƒY", value=1955)
    crop_w = st.number_input("è£å‰ªå®½åº¦(px)", value=200)
    crop_h = st.number_input("è£å‰ªé«˜åº¦(px)", value=50)
    if st.button("å¼€å§‹è£å‰ª+OCR"):
        if folder_path_input and os.path.exists(folder_path_input):
            output_folder, results = crop_and_ocr_images_from_folder(folder_path_input, x_center, y_center, crop_w, crop_h)
            st.success(f"å®Œæˆï¼è£å‰ª+OCRç»“æœå·²ä¿å­˜åˆ°ï¼š{output_folder}")
            st.table(pd.DataFrame(results, columns=["åŸæ–‡ä»¶å", "æ–°æ–‡ä»¶å"]))
        else:
            st.warning("è¯·æä¾›æœ‰æ•ˆå›¾ç‰‡æ–‡ä»¶å¤¹è·¯å¾„")

# ------------------------ å…¶å®ƒ Tabs å¯ç»§ç»­æ”¾ç½®å‰é¢ä»£ç  ------------------------
# Tab1: ç½‘é¡µè¡¨æ ¼æŠ“å–
with tab1:
    st.subheader("ç½‘é¡µè¡¨æ ¼æŠ“å–")
    urls_text = st.text_area("è¾“å…¥ç½‘é¡µURLåˆ—è¡¨ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰", height=160)
    group_cols = st.text_input("åˆ†ç»„åˆ—ï¼ˆé€—å·åˆ†éš”ï¼Œå¯é€‰ï¼‰")
    if st.button("æŠ“å–è¡¨æ ¼", key="scrape_btn"):
        url_list = [u.strip() for u in urls_text.splitlines() if u.strip()]
        if url_list:
            output = scrape_table(url_list, group_cols)
            if output:
                st.download_button("ä¸‹è½½æŠ“å–è¡¨æ ¼", data=output.getvalue(), file_name="ç½‘é¡µæŠ“å–.xlsx")
            else:
                st.warning("æœªæŠ“å–åˆ°è¡¨æ ¼")
        else:
            st.warning("è¯·æä¾›æœ‰æ•ˆURLåˆ—è¡¨")

# Tab2: ç½‘é¡µå›¾ç‰‡ä¸‹è½½
with tab2:
    st.subheader("ç½‘é¡µå›¾ç‰‡ä¸‹è½½")
    urls_text2 = st.text_area("è¾“å…¥ç½‘é¡µURLåˆ—è¡¨ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰", height=160)
    outdir_input = st.text_input("è¾“å‡ºæ–‡ä»¶å¤¹ï¼ˆå¯é€‰ï¼‰")
    if st.button("ä¸‹è½½ç½‘é¡µå›¾ç‰‡", key="img_download_btn"):
        url_list = [u.strip() for u in urls_text2.splitlines() if u.strip()]
        output_dir = outdir_input.strip() or None
        if url_list:
            folder, files = download_images_from_urls(url_list, output_dir)[:2]
            st.success(f"ä¸‹è½½å®Œæˆï¼Œå…± {len(files)} å¼ å›¾ç‰‡ï¼Œä¿å­˜åˆ° {folder}")

# Tab4: é«˜æ ¡é€‰ç§‘è½¬æ¢
with tab4:
    st.subheader("é«˜æ ¡é€‰ç§‘è½¬æ¢")
    uploaded_excel = st.file_uploader("ä¸Šä¼ Excelæ–‡ä»¶", type=["xlsx"])
    if uploaded_excel and st.button("è½¬æ¢é€‰ç§‘", key="sel_btn"):
        df = pd.read_excel(uploaded_excel)
        df_new = convert_selection_requirements(df)
        output = BytesIO()
        df_new.to_excel(output, index=False)
        output.seek(0)
        st.download_button("ä¸‹è½½è½¬æ¢ç»“æœ", data=output.getvalue(), file_name="é€‰ç§‘è½¬æ¢.xlsx")

# Tab5: Excelæ—¥æœŸå¤„ç†
with tab5:
    st.subheader("Excelæ—¥æœŸå¤„ç†")
    uploaded_excel2 = st.file_uploader("ä¸Šä¼ Excelæ–‡ä»¶", type=["xlsx"], key="date_excel")
    year_input = st.number_input("å¹´ä»½ï¼ˆç”¨äºè¡¥å…¨ï¼‰", value=datetime.now().year)
    date_col = st.text_input("æ—¥æœŸåˆ—å", value="æ—¥æœŸ")
    if uploaded_excel2 and st.button("å¤„ç†æ—¥æœŸ", key="date_btn"):
        df = pd.read_excel(uploaded_excel2)
        start_times, end_times, originals = [], [], []
        for d in progress_iter(list(df[date_col]), text="æ—¥æœŸå¤„ç†ä¸­"):
            orig, start, end = process_date_range(d, year_input)
            originals.append(orig)
            start_times.append(start)
            end_times.append(end)
        df_result = df.copy()
        insert_at = df_result.columns.get_loc(date_col) + 1
        df_result.insert(insert_at, 'å¼€å§‹æ—¶é—´', start_times)
        df_result.insert(insert_at + 1, 'ç»“æŸæ—¶é—´', end_times)
        output = BytesIO()
        df_result.to_excel(output, index=False)
        output.seek(0)
        st.download_button("ä¸‹è½½æ—¥æœŸå¤„ç†ç»“æœExcel", data=output.getvalue(), file_name="æ—¥æœŸå¤„ç†ç»“æœ.xlsx")

# Tab6: è¿è¡Œæ—¥å¿—
with tab6:
    st.subheader("è¿è¡Œæ—¥å¿—ï¼ˆæœ€æ–°ï¼‰")
    for line in st.session_state.recent_logs[-200:]:
        st.text(line)

st.caption("è¯´æ˜ï¼šè£å‰ªç»“æœå’ŒOCRé‡å‘½åä¿å­˜åˆ°æ¡Œé¢ crop_results æ–‡ä»¶å¤¹ï¼Œä¸‹è½½ç½‘é¡µå›¾ç‰‡é»˜è®¤ä¿å­˜åˆ°æ¡Œé¢ downloaded_images æ–‡ä»¶å¤¹")
